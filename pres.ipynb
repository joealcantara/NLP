{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pres.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTU4SFVH3LSredvSMxswiH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joealcantara/NLP/blob/master/pres.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKSAQWpQznU6",
        "colab_type": "text"
      },
      "source": [
        "Presidents Code - Move to Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IDq01ZuyfCh",
        "colab_type": "code",
        "outputId": "d612e69c-2126-4eaf-91bc-7974e9734309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79kYzPvazx0o",
        "colab_type": "code",
        "outputId": "3fceee74-10d7-47f7-9eed-b1ca41934b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "from nltk.stem import *\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initalise stemmer and lemmatizer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from datetime import date\n",
        "\n",
        "# To show plots in notebook\n",
        "%matplotlib inline  \n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
        "pd.set_option('mode.chained_assignment','warn')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEorG2WX228j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wordCount(text):\n",
        "    wordDict = {}\n",
        "    for sentence in text:\n",
        "        for entry in sentence:\n",
        "            keyTitle = entry[1]\n",
        "            if keyTitle not in wordDict:\n",
        "                wordDict[keyTitle] = 1\n",
        "            else:\n",
        "                count = wordDict[keyTitle]\n",
        "                count = count + 1\n",
        "                wordDict[keyTitle] = count\n",
        "    return wordDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CN7Zu3DDcNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def meanLengthSentence(text):\n",
        "    total_length = 0\n",
        "    for sent in text:\n",
        "        total_length = total_length+len(sent)\n",
        "    return total_length / len(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab43F_f1DhlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentences = [word_tokenize(sent) for sent in sentences]\n",
        "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZRVKk0Dr2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_punctuation(text):\n",
        "    table = str.maketrans(dict.fromkeys(':,.?'))\n",
        "    s = text.translate(table)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bZrIGpiNEez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lexical_diversity(text):\n",
        "    return len(set(text)) / len(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XTSuXT_JdvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/Data/presidents/data/\"\n",
        "\n",
        "pathReagan = path + 'ReaganSpeeches/'\n",
        "pathBush = path + 'BushSpeeches/'\n",
        "pathTrump = path + 'TrumpSpeeches/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTjjrENULxg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Data Frames for the datasets. I am including one data frame for 2 terms of Reagan\n",
        "# and 2 separate dataframes for each term.\n",
        "dfReagan = pd.DataFrame()\n",
        "dfReaganTerm1 = pd.DataFrame()\n",
        "dfReaganTerm2 = pd.DataFrame()\n",
        "dfBush = pd.DataFrame()\n",
        "dfTrump = pd.DataFrame()\n",
        "LIWC = pd.DataFrame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRu1Q6qML3IM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for filename in os.listdir(pathReagan):\n",
        "    if filename.endswith('txt'):\n",
        "        f = open(pathReagan + filename)\n",
        "        raw = f.read()\n",
        "        # Clear raw of punctuation and tokenize for word counts.\n",
        "        wordsNoPunct = strip_punctuation(raw)\n",
        "        #hesitations = wordsNoPunct.count('—')\n",
        "        wordsNoPunct.replace(\"—\", ' ')\n",
        "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
        "        words = word_tokenize(raw)\n",
        "        tokens_stemmed = [stemmer.stem(x) for x in words]\n",
        "        newlist = []\n",
        "        for word in words:\n",
        "          x = lemmatizer.lemmatize(word)\n",
        "          newlist.append(x)\n",
        "        UniqueLemmas = len(set(newlist))\n",
        "    \n",
        "        # Word Counts for certain words\n",
        "        c = Counter(words)\n",
        "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
        "        NSNouns = c['something'] + c['anything'] + c['thing'] + c['everything']\n",
        "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
        "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
        "        \n",
        "        sents = sent_tokenize(raw)\n",
        "        processed = preprocess(raw)\n",
        "        lex = lexical_diversity(wordsNoPunct)\n",
        "        mls = meanLengthSentence(processed)\n",
        "        wordDict = wordCount(processed)\n",
        "        thetuple = {'Filename': filename, 'TTR': lex,\n",
        "                    'WordCount':len(wordsNoPunct), \n",
        "                    'UniqueWords':len(set(wordsNoPunct)),\n",
        "                    'UniqueStems':len(set(tokens_stemmed)),\n",
        "                    'UniqueLemmas':UniqueLemmas,\n",
        "                    'MLU': mls, 'Fillers': Fillers,\n",
        "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
        "        finalDict = {**thetuple, **wordDict}\n",
        "        dfReagan = dfReagan.append(finalDict, ignore_index = True)\n",
        "        reaganWords = words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8gVi2ubNwqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for filename in os.listdir(pathBush):\n",
        "    if filename.endswith('txt'):\n",
        "        f = open(pathBush + filename)\n",
        "        raw = f.read()\n",
        "        # Clear raw of punctuation and tokenize for word counts.\n",
        "        wordsNoPunct = strip_punctuation(raw)\n",
        "        #hesitations = wordsNoPunct.count('-')\n",
        "        wordsNoPunct.replace(\"-\", ' ')\n",
        "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
        "        \n",
        "        words = word_tokenize(raw)\n",
        "        tokens_stemmed = [stemmer.stem(x) for x in words]\n",
        "        newlist = []\n",
        "        for word in words:\n",
        "          x = lemmatizer.lemmatize(word)\n",
        "          newlist.append(x)\n",
        "        UniqueLemmas = len(set(newlist))\n",
        "\n",
        "        # Word Counts for certain words\n",
        "        c = Counter(words)\n",
        "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
        "        NSNouns = c['something'] + c['anything'] + c['thing']\n",
        "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
        "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
        "        \n",
        "        sents = sent_tokenize(raw)\n",
        "        processed = preprocess(raw)\n",
        "        lex = lexical_diversity(wordsNoPunct)\n",
        "        mls = meanLengthSentence(processed)\n",
        "        wordDict = wordCount(processed)\n",
        "        thetuple = {'Filename': filename, 'TTR': lex,\n",
        "                    'WordCount':len(wordsNoPunct), \n",
        "                    'UniqueWords':len(set(wordsNoPunct)),\n",
        "                    'UniqueStems':len(set(tokens_stemmed)),\n",
        "                    'UniqueLemmas':UniqueLemmas,\n",
        "                    'MLU': mls, 'Fillers': Fillers,\n",
        "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
        "        finalDict = {**thetuple, **wordDict}\n",
        "        dfBush = dfBush.append(finalDict, ignore_index = True)\n",
        "        bushWords = words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Epwf3JNzvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for filename in os.listdir(pathTrump):\n",
        "    if filename.endswith('txt'):\n",
        "        f = open(pathTrump + filename)\n",
        "        raw = f.read()\n",
        "        # Clear raw of punctuation and tokenize for word counts.\n",
        "        wordsNoPunct = strip_punctuation(raw)\n",
        "        #hesitations = wordsNoPunct.count('-')\n",
        "        wordsNoPunct.replace(\"-\", ' ')\n",
        "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
        "        \n",
        "        words = word_tokenize(raw)\n",
        "        tokens_stemmed = [stemmer.stem(x) for x in words]\n",
        "        newlist = []\n",
        "        for word in words:\n",
        "          x = lemmatizer.lemmatize(word)\n",
        "          newlist.append(x)\n",
        "        UniqueLemmas = len(set(newlist))\n",
        "\n",
        "        # Word Counts for certain words\n",
        "        c = Counter(words)\n",
        "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
        "        NSNouns = c['something'] + c['anything'] + c['thing']\n",
        "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
        "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
        "        \n",
        "        sents = sent_tokenize(raw)\n",
        "        processed = preprocess(raw)\n",
        "        lex = lexical_diversity(wordsNoPunct)\n",
        "        mls = meanLengthSentence(processed)\n",
        "        wordDict = wordCount(processed)\n",
        "        thetuple = {'Filename': filename, 'TTR': lex,\n",
        "                    'WordCount':len(wordsNoPunct), \n",
        "                    'UniqueWords':len(set(wordsNoPunct)),\n",
        "                    'UniqueStems':len(set(tokens_stemmed)),\n",
        "                    'UniqueLemmas':UniqueLemmas,\n",
        "                    'MLU': mls, 'Fillers': Fillers,\n",
        "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
        "        finalDict = {**thetuple, **wordDict}\n",
        "        dfTrump = dfTrump.append(finalDict, ignore_index = True)\n",
        "        trumpWords = words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I81R79GyOc92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LIWC = pd.read_csv(path + \"LIWC2015Results.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYVv6kF6ZHeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfReagan = pd.merge(dfReagan, LIWC, on='Filename', how='inner')\n",
        "dfTrump = pd.merge(dfTrump, LIWC, on='Filename', how='inner')\n",
        "dfBush = pd.merge(dfBush, LIWC, on='Filename', how='inner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDW4i_ICZUlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rearranging Columns\n",
        "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'UniqueStems', 'UniqueLemmas', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
        "cols = ([col for col in inserted_cols if col in dfReagan] \n",
        "        + [col for col in dfReagan if col not in inserted_cols])\n",
        "dfReagan = dfReagan[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtb3RmgZZWdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rearranging Columns\n",
        "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'UniqueStems', 'UniqueLemmas', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
        "cols = ([col for col in inserted_cols if col in dfBush] \n",
        "        + [col for col in dfBush if col not in inserted_cols])\n",
        "dfBush = dfBush[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fO0uYfSZYCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rearranging Columns\n",
        "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'UniqueStems', 'UniqueLemmas', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
        "cols = ([col for col in inserted_cols if col in dfTrump] \n",
        "        + [col for col in dfTrump if col not in inserted_cols])\n",
        "dfTrump = dfTrump[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYoitd3ybQUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ReaganDates = pd.read_csv(path + \"ReaganDates.csv\")\n",
        "BushDates = pd.read_csv(path + \"BushDates.csv\")\n",
        "TrumpDates = pd.read_csv(path + \"TrumpDates.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUUKfZUbbyZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "ReaganDates['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in ReaganDates['Date']]\n",
        "BushDates['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in BushDates['Date']]\n",
        "TrumpDates['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in TrumpDates['Date']]\n",
        "da = datetime.datetime(1981, 1, 29)\n",
        "da = np.datetime64(da)\n",
        "dc = datetime.datetime(1989, 1, 27)\n",
        "dc = np.datetime64(dc)\n",
        "de = datetime.datetime(2017, 1, 27)\n",
        "de = np.datetime64(de)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAdhMjqzcvsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ReaganDates['Days'] = ReaganDates['JDate'] - da\n",
        "ReaganDates['Days'] = ReaganDates['Days'].dt.days\n",
        "BushDates['Days'] = BushDates['JDate'] - dc\n",
        "BushDates['Days'] = BushDates['Days'].dt.days\n",
        "TrumpDates['Days'] = TrumpDates['JDate'] - de\n",
        "TrumpDates['Days'] = TrumpDates['Days'].dt.days"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AmgCKnqdLQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfReagan = ReaganDates.merge(dfReagan, on='Filename')\n",
        "dfBush = BushDates.merge(dfBush, on='Filename')\n",
        "dfTrump = TrumpDates.merge(dfTrump, on='Filename')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgfHhaCidepJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfReagan = dfReagan.sort_values(by=['Days'])\n",
        "dfBush = dfBush.sort_values(by=['Days'])\n",
        "dfTrump = dfTrump.sort_values(by=['Days'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t19o0_xpd2NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate some new aggregate columns\n",
        "dfReagan['Nouns'] = dfReagan['NN'] + dfReagan['NNS']+ dfReagan['NNP'] + dfReagan['NNPS']\n",
        "dfReagan['Nouns/100'] = dfReagan['Nouns'] / 100\n",
        "dfReagan['NounsNormalised'] = dfReagan['Nouns'] / dfReagan['WordCount']\n",
        "dfReagan['Adjectives'] = dfReagan['JJ'] + dfReagan['JJR'] + dfReagan['JJS']\n",
        "dfReagan['Adjectives/100'] = dfReagan['Adjectives'] / 100\n",
        "dfReagan['AdjectivesNormalised'] = dfReagan['Adjectives'] / dfReagan['WordCount']\n",
        "dfReagan['Adverbs'] = dfReagan['RB'] + dfReagan['RBR'] + dfReagan['RBS']\n",
        "dfReagan['Adverbs/100'] = dfReagan['Adverbs'] / 100\n",
        "dfReagan['AdverbsNormalised'] = dfReagan['Adverbs'] / dfReagan['WordCount']\n",
        "dfReagan['Verbs'] = dfReagan['VB'] + dfReagan['VBD'] + dfReagan['VBG'] + dfReagan['VBN'] + dfReagan['VBP'] + dfReagan['VBZ']\n",
        "dfReagan['Verbs/100'] = dfReagan['Verbs'] / 100\n",
        "dfReagan['VerbsNormalised'] = dfReagan['Verbs'] / dfReagan['WordCount']\n",
        "dfReagan['Pronouns'] = dfReagan['PRP'] + dfReagan['PRP$']\n",
        "dfReagan['PronounsNormalised'] = dfReagan['Pronouns'] / dfReagan['WordCount']\n",
        "dfReagan['UniqueWordsNormalised'] = dfReagan['UniqueWords'] / dfReagan['WordCount']\n",
        "dfReagan['UniqueStemsNormalised'] = dfReagan['UniqueStems'] / dfReagan['WordCount']\n",
        "dfReagan['UniqueLemmasNormalised'] = dfReagan['UniqueLemmas'] / dfReagan['WordCount']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT9AAGacd4xW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate some new aggregate columns\n",
        "dfBush['Nouns'] = dfBush['NN'] + dfBush['NNS']+ dfBush['NNP'] + dfBush['NNPS']\n",
        "dfBush['Nouns/100'] = dfBush['Nouns'] / 100\n",
        "dfBush['NounsNormalised'] = dfBush['Nouns'] / dfBush['WordCount']\n",
        "dfBush['Adjectives'] = dfBush['JJ'] + dfBush['JJR'] + dfBush['JJS']\n",
        "dfBush['Adjectives/100'] = dfBush['Adjectives'] / 100\n",
        "dfBush['AdjectivesNormalised'] = dfBush['Adjectives'] / dfBush['WordCount']\n",
        "dfBush['Adverbs'] = dfBush['RB'] + dfBush['RBR'] + dfBush['RBS']\n",
        "dfBush['Adverbs/100'] = dfBush['Adverbs'] / 100\n",
        "dfBush['AdverbsNormalised'] = dfBush['Adverbs'] / dfBush['WordCount']\n",
        "dfBush['Verbs'] = dfBush['VB'] + dfBush['VBD'] + dfBush['VBG'] + dfBush['VBN'] + dfBush['VBP'] + dfBush['VBZ']\n",
        "dfBush['Verbs/100'] = dfBush['Verbs'] / 100\n",
        "dfBush['VerbsNormalised'] = dfBush['Verbs'] / dfBush['WordCount']\n",
        "dfBush['Pronouns'] = dfBush['PRP'] + dfBush['PRP$']\n",
        "dfBush['PronounsNormalised'] = dfBush['Pronouns'] / dfBush['WordCount']\n",
        "dfBush['UniqueWordsNormalised'] = dfReagan['UniqueWords'] / dfReagan['WordCount']\n",
        "dfBush['UniqueStemsNormalised'] = dfReagan['UniqueStems'] / dfReagan['WordCount']\n",
        "dfBush['UniqueLemmasNormalised'] = dfBush['UniqueLemmas'] / dfBush['WordCount']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkFZlJ71d7B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate some new aggregate columns\n",
        "dfTrump['Nouns'] = dfTrump['NN'] + dfTrump['NNS']+ dfTrump['NNP'] + dfTrump['NNPS']\n",
        "dfTrump['Nouns/100'] = dfTrump['Nouns'] / 100\n",
        "dfTrump['NounsNormalised'] = dfTrump['Nouns'] / dfTrump['WordCount']\n",
        "dfTrump['Adjectives'] = dfTrump['JJ'] + dfTrump['JJR'] + dfTrump['JJS']\n",
        "dfTrump['Adjectives/100'] = dfTrump['Adjectives'] / 100\n",
        "dfTrump['AdjectivesNormalised'] = dfTrump['Adjectives'] / dfTrump['WordCount']\n",
        "dfTrump['Adverbs'] = dfTrump['RB'] + dfTrump['RBR'] + dfTrump['RBS']\n",
        "dfTrump['Adverbs/100'] = dfTrump['Adverbs'] / 100\n",
        "dfTrump['AdverbsNormalised'] = dfTrump['Adverbs'] / dfTrump['WordCount']\n",
        "dfTrump['Verbs'] = dfTrump['VB'] + dfTrump['VBD'] + dfTrump['VBG'] + dfTrump['VBN'] + dfTrump['VBP'] + dfTrump['VBZ']\n",
        "dfTrump['Verbs/100'] = dfTrump['Verbs'] / 100\n",
        "dfTrump['VerbsNormalised'] = dfTrump['Verbs'] / dfTrump['WordCount']\n",
        "dfTrump['Pronouns'] = dfTrump['PRP'] + dfTrump['PRP$']\n",
        "dfTrump['PronounsNormalised'] = dfTrump['Pronouns'] / dfTrump['WordCount']\n",
        "dfTrump['UniqueWordsNormalised'] = dfTrump['UniqueWords'] / dfTrump['WordCount']\n",
        "dfTrump['UniqueStemsNormalised'] = dfTrump['UniqueStems'] / dfTrump['WordCount']\n",
        "dfTrump['UniqueLemmasNormalised'] = dfTrump['UniqueLemmas'] / dfTrump['WordCount']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3lhpo8meALm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfReaganTerm1 = dfReagan[dfReagan.JDate < pd.Timestamp(1985, 1, 31)]\n",
        "dfReaganTerm2 = dfReagan[dfReagan.JDate > pd.Timestamp(1985, 1, 31)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMUDKuA0gd69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resultsReagan = pd.DataFrame()\n",
        "columnsReagan = list(dfReagan)\n",
        "resultsBush = pd.DataFrame()\n",
        "columnsBush = list(dfBush)\n",
        "resultsTrump = pd.DataFrame()\n",
        "columnsTrump = list(dfTrump)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSmLW5gZgmQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columnsReagan.remove('Filename')\n",
        "columnsReagan.remove('Date')\n",
        "columnsReagan.remove('JDate')\n",
        "\n",
        "columnsBush.remove('Filename')\n",
        "columnsBush.remove('Date')\n",
        "columnsBush.remove('JDate')\n",
        "\n",
        "columnsTrump.remove('Filename')\n",
        "columnsTrump.remove('Date')\n",
        "columnsTrump.remove('JDate')\n",
        "\n",
        "# Fill NA's with 0s as in this dataset, NAN represent the feature NOT occuring in a particular document.\n",
        "dfReagan = dfReagan.fillna(0)\n",
        "dfBush = dfBush.fillna(0)\n",
        "dfTrump = dfTrump.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYN9NB7Ygv5I",
        "colab_type": "code",
        "outputId": "1354ea3e-eba1-4e80-b83b-4e513d789efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "for i in columnsReagan:\n",
        "    r, p = pearsonr(dfReagan[i], dfReagan['Days'])\n",
        "    pearsonResults = {'Feature': i, 'RSquared':r, 'P-Value': p}\n",
        "    resultsReagan = resultsReagan.append(pearsonResults, ignore_index=True)\n",
        "\n",
        "for i in columnsTrump:\n",
        "    r, p = pearsonr(dfTrump[i], dfTrump['Days'])\n",
        "    pearsonResults = {'Feature': i, 'RSquared':r, 'P-Value': p}\n",
        "    resultsTrump = resultsTrump.append(pearsonResults, ignore_index=True)\n",
        "\n",
        "for i in columnsBush:\n",
        "    r, p = pearsonr(dfBush[i], dfBush['Days'])\n",
        "    pearsonResults = {'Feature': i, 'RSquared':r, 'P-Value': p}\n",
        "    resultsBush = resultsBush.append(pearsonResults, ignore_index=True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QsHSP-zTfgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processForFDR(df):\n",
        "  df = df.dropna()\n",
        "  indexes = df[df['RSquared'] == 1].index\n",
        "  df.drop(indexes, inplace=True)  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmeiHV_hB7TU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "32a6172c-95af-4280-feaf-fba3a35e4e7c"
      },
      "source": [
        "resultsReagan = processForFDR(resultsReagan)\n",
        "resultsBush = processForFDR(resultsBush)\n",
        "resultsTrump = processForFDR(resultsTrump)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afp183X0SfB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.stats.multitest import (multipletests, fdrcorrection,\n",
        "                                         fdrcorrection_twostage,\n",
        "                                         NullDistribution,\n",
        "                                         local_fdr)\n",
        "\n",
        "test = multipletests(resultsReagan['P-Value'], alpha=0.05, method = 'bonferroni', is_sorted=False, returnsorted=False)\n",
        "resultsReagan['BN Accepted'] = test[0]\n",
        "resultsReagan['BN Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsReagan['P-Value'], alpha=0.05, method = 'holm', is_sorted=False, returnsorted=False)\n",
        "resultsReagan['Holm Accepted'] = test[0]\n",
        "resultsReagan['Holm Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsReagan['P-Value'], alpha=0.05, method = 'fdr_bh', is_sorted=False, returnsorted=False)\n",
        "resultsReagan['BH Accepted'] = test[0]\n",
        "resultsReagan['BH Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsReagan['P-Value'], alpha=0.05, method = 'fdr_by', is_sorted=False, returnsorted=False)\n",
        "resultsReagan['BY Accepted'] = test[0]\n",
        "resultsReagan['BY Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsReagan['P-Value'], alpha=0.05, method = 'hommel', is_sorted=False, returnsorted=False)\n",
        "resultsReagan['HM Accepted'] = test[0]\n",
        "resultsReagan['HM Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsReagan['P-Value'], alpha=0.05, method = 'simes-hochberg', is_sorted=False, returnsorted=False)\n",
        "resultsReagan['HO Accepted'] = test[0]\n",
        "resultsReagan['HO Adjusted P-value'] = test[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkDlKMq_D8aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = multipletests(resultsBush['P-Value'], alpha=0.05, method = 'bonferroni', is_sorted=False, returnsorted=False)\n",
        "resultsBush['BN Accepted'] = test[0]\n",
        "resultsBush['BN Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsBush['P-Value'], alpha=0.05, method = 'holm', is_sorted=False, returnsorted=False)\n",
        "resultsBush['Holm Accepted'] = test[0]\n",
        "resultsBush['Holm Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsBush['P-Value'], alpha=0.05, method = 'fdr_bh', is_sorted=False, returnsorted=False)\n",
        "resultsBush['BH Accepted'] = test[0]\n",
        "resultsBush['BH Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsBush['P-Value'], alpha=0.05, method = 'fdr_by', is_sorted=False, returnsorted=False)\n",
        "resultsBush['BY Accepted'] = test[0]\n",
        "resultsBush['BY Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsBush['P-Value'], alpha=0.05, method = 'hommel', is_sorted=False, returnsorted=False)\n",
        "resultsBush['HM Accepted'] = test[0]\n",
        "resultsBush['HM Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsBush['P-Value'], alpha=0.05, method = 'simes-hochberg', is_sorted=False, returnsorted=False)\n",
        "resultsBush['HO Accepted'] = test[0]\n",
        "resultsBush['HO Adjusted P-value'] = test[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6EgY668D8h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = multipletests(resultsTrump['P-Value'], alpha=0.05, method = 'bonferroni', is_sorted=False, returnsorted=False)\n",
        "resultsTrump['BN Accepted'] = test[0]\n",
        "resultsTrump['BN Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsTrump['P-Value'], alpha=0.05, method = 'holm', is_sorted=False, returnsorted=False)\n",
        "resultsTrump['Holm Accepted'] = test[0]\n",
        "resultsTrump['Holm Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsTrump['P-Value'], alpha=0.05, method = 'fdr_bh', is_sorted=False, returnsorted=False)\n",
        "resultsTrump['BH Accepted'] = test[0]\n",
        "resultsTrump['BH Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsTrump['P-Value'], alpha=0.05, method = 'fdr_by', is_sorted=False, returnsorted=False)\n",
        "resultsTrump['BY Accepted'] = test[0]\n",
        "resultsTrump['BY Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsTrump['P-Value'], alpha=0.05, method = 'hommel', is_sorted=False, returnsorted=False)\n",
        "resultsTrump['HM Accepted'] = test[0]\n",
        "resultsTrump['HM Adjusted P-value'] = test[1]\n",
        "\n",
        "test = multipletests(resultsTrump['P-Value'], alpha=0.05, method = 'simes-hochberg', is_sorted=False, returnsorted=False)\n",
        "resultsTrump['HO Accepted'] = test[0]\n",
        "resultsTrump['HO Adjusted P-value'] = test[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmfm4Vb5hAvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subsetReagan = pd.DataFrame()\n",
        "subsetReagan = resultsReagan.loc[resultsReagan['P-Value'] < 0.05]\n",
        "subsetReagan1 = resultsReagan.loc[resultsReagan['RSquared'] < -0.40]\n",
        "subsetReagan2 = resultsReagan.loc[resultsReagan['RSquared'] > 0.40]\n",
        "subsetReagan1 = subsetReagan1.sort_values(by=['RSquared'])\n",
        "subsetReagan2 = subsetReagan2.sort_values(by=['RSquared'])\n",
        "subsetReagan1.append(subsetReagan2)\n",
        "ReaganNames = subsetReagan1['Feature'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rg-bitcGjkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subsetBush = resultsBush.loc[resultsBush['P-Value'] < 0.05]\n",
        "subsetBush1 = resultsBush.loc[resultsBush['RSquared'] < -0.40]\n",
        "subsetBush2 = resultsBush.loc[resultsBush['RSquared'] > 0.40]\n",
        "subsetBush1 = subsetBush1.sort_values(by=['RSquared'])\n",
        "subsetBush2 = subsetBush2.sort_values(by=['RSquared'])\n",
        "subsetBush1.append(subsetBush2)\n",
        "BushNames = subsetBush1['Feature'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtWiJUxRGpcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subsetTrump = resultsTrump.loc[resultsTrump['P-Value'] < 0.05]\n",
        "subsetTrump1 = resultsTrump.loc[resultsTrump['RSquared'] < -0.40]\n",
        "subsetTrump2 = resultsTrump.loc[resultsTrump['RSquared'] > 0.40]\n",
        "subsetTrump1 = subsetTrump1.sort_values(by=['RSquared'])\n",
        "subsetTrump2 = subsetTrump2.sort_values(by=['RSquared'])\n",
        "subsetTrump1.append(subsetTrump2)\n",
        "TrumpNames = subsetTrump1['Feature'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqFntrw96o98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name in ReaganNames:\n",
        "  fig = px.scatter(dfReagan, x = \"Days\", y = name, trendline=\"ols\")\n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-6knN0K7c9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name in BushNames:\n",
        "  fig = px.scatter(dfBush, x = \"Days\", y = name, trendline=\"ols\")\n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt20ckrN7jyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name in TrumpNames:\n",
        "  fig = px.scatter(dfTrump, x = \"Days\", y = name, trendline=\"ols\", title='test')\n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDx-NPa70HE7",
        "colab_type": "code",
        "outputId": "ca2e7736-c14e-40c7-c982-e0204127fa2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!pip install pygam\n",
        "from pygam import LogisticGAM, LinearGAM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygam in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pygam) (1.4.1)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from pygam) (3.38.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pygam) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pygam) (1.18.3)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->pygam) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2->pygam) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvOADpId0Xz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}