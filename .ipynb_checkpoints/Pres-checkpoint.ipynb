{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages I will need\n",
    "from lex_processing import * \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To show plots in notebook\n",
    "%matplotlib inline  \n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('classic')\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path is where the data I want to process is.\n",
    "# For Mac\n",
    "pathReagan = '/Users/Joe/dropbox/Data/Edited Data/Presidents Data/ReaganSpeeches/'\n",
    "pathBush = '/Users/Joe/dropbox/Data/Edited Data/Presidents Data/BushSpeeches/'\n",
    "pathTrump = '/Users/Joe/dropbox/Data/Edited Data/Presidents Data/TrumpSpeeches/'\n",
    "# For Linux\n",
    "#pathReagan = '/home/CAMPUS/alcantaj/Dropbox/Data/Edited Data/Presidents Data/ReaganSpeeches/'\n",
    "#pathBush = '/home/CAMPUS/alcantaj/Dropbox/Data/Edited Data/Presidents Data/BushSpeeches/'\n",
    "#pathTrump = '/home/CAMPUS/alcantaj/Dropbox/Data/Edited Data/Presidents Data/TrumpSpeeches/'\n",
    "# For Windows\n",
    "# pathReagan = '/Users/jomar/Dropbox/Data/Edited Data/Presidents Data/ReaganSpeeches/'\n",
    "#pathBush = '/Users/jomar/Dropbox/Data/Edited Data/Presidents Data/BushSpeeches/'\n",
    "#pathTrump = '/Users/jomar/Dropbox/Data/Edited Data/Presidents Data/TrumpSpeeches/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Frames for the datasets. I am including one data frame for 2 terms of Reagan\n",
    "# and 2 separate dataframes for each term.\n",
    "dfReagan = pd.DataFrame()\n",
    "dfReaganTerm1 = pd.DataFrame()\n",
    "dfReaganTerm2 = pd.DataFrame()\n",
    "dfBush = pd.DataFrame()\n",
    "dfTrump = pd.DataFrame()\n",
    "LIWC = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathReagan):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathReagan + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('—')\n",
    "        wordsNoPunct.replace(\"—\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        words = word_tokenize(raw)\n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing'] + c['everything']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)), \n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfReagan = dfReagan.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathBush):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathBush + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('-')\n",
    "        wordsNoPunct.replace(\"-\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        \n",
    "        words = word_tokenize(raw)\n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)), \n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfBush = dfBush.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathTrump):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathTrump + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('-')\n",
    "        wordsNoPunct.replace(\"-\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        \n",
    "        words = word_tokenize(raw)\n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)), \n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfTrump = dfTrump.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mac\n",
    "testpath = '/Users/Joe/Documents/NLP/'\n",
    "# For Linux\n",
    "# testpath = '/home/CAMPUS/alcantaj/Documents/NLP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC = pd.read_csv(testpath + \"LIWC2015Results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReagan = pd.merge(dfReagan, LIWC, on='Filename', how='inner')\n",
    "dfTrump = pd.merge(dfTrump, LIWC, on='Filename', how='inner')\n",
    "dfBush = pd.merge(dfBush, LIWC, on='Filename', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns\n",
    "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
    "cols = ([col for col in inserted_cols if col in dfReagan] \n",
    "        + [col for col in dfReagan if col not in inserted_cols])\n",
    "dfReagan = dfReagan[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns\n",
    "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
    "cols = ([col for col in inserted_cols if col in dfBush] \n",
    "        + [col for col in dfBush if col not in inserted_cols])\n",
    "dfBush = dfBush[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Columns\n",
    "inserted_cols = ['Filename', 'TTR','WordCount', 'UniqueWords', 'MLU', 'Fillers', 'NSNouns', 'LIVerbs']\n",
    "cols = ([col for col in inserted_cols if col in dfTrump] \n",
    "        + [col for col in dfTrump if col not in inserted_cols])\n",
    "dfTrump = dfTrump[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA's with 0s as in this dataset, NAN represent the feature NOT occuring in a particular document.\n",
    "dfReagan = dfReagan.fillna(0)\n",
    "dfBush = dfBush.fillna(0)\n",
    "dfTrump = dfTrump.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output \n",
    "dfReagan.to_csv('testReagan.csv')\n",
    "dfBush.to_csv('testBush.csv')\n",
    "dfTrump.to_csv('testTrump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(testpath + \"dates.csv\")\n",
    "b = pd.read_csv(testpath + \"testReagan.csv\")\n",
    "\n",
    "c = pd.read_csv(testpath + \"dates2.csv\")\n",
    "d = pd.read_csv(testpath + \"testBush.csv\")\n",
    "\n",
    "e = pd.read_csv(testpath + \"dates3.csv\")\n",
    "f = pd.read_csv(testpath + \"testTrump.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in a['Date']]\n",
    "c['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in c['Date']]\n",
    "e['JDate'] = [datetime.datetime.strptime(x, '%d/%m/%Y') for x in e['Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Julian'] = [get_julian_datetime(x) for x in a['JDate']]\n",
    "c['Julian'] = [get_julian_datetime(x) for x in c['JDate']]\n",
    "e['Julian'] = [get_julian_datetime(x) for x in e['JDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReagan = a.merge(b, on='Filename')\n",
    "dfBush = c.merge(d, on='Filename')\n",
    "dfTrump = e.merge(f, on='Filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReagan = dfReagan.sort_values(by=['JDate'])\n",
    "dfBush = dfBush.sort_values(by=['JDate'])\n",
    "dfTrump = dfTrump.sort_values(by=['JDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new labels (Index is in order of article date)\n",
    "dfReagan = dfReagan.reset_index()\n",
    "dfReagan['index'] = dfReagan.index\n",
    "dfBush = dfBush.reset_index()\n",
    "dfBush['index'] = dfBush.index\n",
    "dfTrump = dfTrump.reset_index()\n",
    "dfTrump['index'] = dfTrump.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Redundant Columns\n",
    "dfReagan = dfReagan.drop(['Unnamed: 0'], axis=1)\n",
    "dfBush = dfBush.drop(['Unnamed: 0'], axis=1)\n",
    "dfTrump = dfTrump.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfReagan['Nouns'] = dfReagan['NN'] + dfReagan['NNS']+ dfReagan['NNP'] + dfReagan['NNPS']\n",
    "dfReagan['Nouns/100'] = dfReagan['Nouns'] / 100\n",
    "dfReagan['NounsNormalised'] = dfReagan['Nouns'] / dfReagan['WordCount']\n",
    "dfReagan['Adjectives'] = dfReagan['JJ'] + dfReagan['JJR'] + dfReagan['JJS']\n",
    "dfReagan['Adjectives/100'] = dfReagan['Adjectives'] / 100\n",
    "dfReagan['AdjectivesNormalised'] = dfReagan['Adjectives'] / dfReagan['WordCount']\n",
    "dfReagan['Adverbs'] = dfReagan['RB'] + dfReagan['RBR'] + dfReagan['RBS']\n",
    "dfReagan['Adverbs/100'] = dfReagan['Adverbs'] / 100\n",
    "dfReagan['AdverbsNormalised'] = dfReagan['Adverbs'] / dfReagan['WordCount']\n",
    "dfReagan['Verbs'] = dfReagan['VB'] + dfReagan['VBD'] + dfReagan['VBG'] + dfReagan['VBN'] + dfReagan['VBP'] + dfReagan['VBZ']\n",
    "dfReagan['Verbs/100'] = dfReagan['Verbs'] / 100\n",
    "dfReagan['VerbsNormalised'] = dfReagan['Verbs'] / dfReagan['WordCount']\n",
    "dfReagan['Pronouns'] = dfReagan['PRP'] + dfReagan['PRP$']\n",
    "dfReagan['PronounsNormalised'] = dfReagan['Pronouns'] / dfReagan['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfBush['Nouns'] = dfBush['NN'] + dfBush['NNS']+ dfBush['NNP'] + dfBush['NNPS']\n",
    "dfBush['Nouns/100'] = dfBush['Nouns'] / 100\n",
    "dfBush['NounsNormalised'] = dfBush['Nouns'] / dfBush['WordCount']\n",
    "dfBush['Adjectives'] = dfBush['JJ'] + dfBush['JJR'] + dfBush['JJS']\n",
    "dfBush['Adjectives/100'] = dfBush['Adjectives'] / 100\n",
    "dfBush['AdjectivesNormalised'] = dfBush['Adjectives'] / dfBush['WordCount']\n",
    "dfBush['Adverbs'] = dfBush['RB'] + dfBush['RBR'] + dfBush['RBS']\n",
    "dfBush['Adverbs/100'] = dfBush['Adverbs'] / 100\n",
    "dfBush['AdverbsNormalised'] = dfBush['Adverbs'] / dfBush['WordCount']\n",
    "dfBush['Verbs'] = dfBush['VB'] + dfBush['VBD'] + dfBush['VBG'] + dfBush['VBN'] + dfBush['VBP'] + dfBush['VBZ']\n",
    "dfBush['Verbs/100'] = dfBush['Verbs'] / 100\n",
    "dfBush['VerbsNormalised'] = dfBush['Verbs'] / dfBush['WordCount']\n",
    "dfBush['Pronouns'] = dfBush['PRP'] + dfBush['PRP$']\n",
    "dfBush['PronounsNormalised'] = dfBush['Pronouns'] / dfBush['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfTrump['Nouns'] = dfTrump['NN'] + dfTrump['NNS']+ dfTrump['NNP'] + dfTrump['NNPS']\n",
    "dfTrump['Nouns/100'] = dfTrump['Nouns'] / 100\n",
    "dfTrump['NounsNormalised'] = dfTrump['Nouns'] / dfTrump['WordCount']\n",
    "dfTrump['Adjectives'] = dfTrump['JJ'] + dfTrump['JJR'] + dfTrump['JJS']\n",
    "dfTrump['Adjectives/100'] = dfTrump['Adjectives'] / 100\n",
    "dfTrump['AdjectivesNormalised'] = dfTrump['Adjectives'] / dfTrump['WordCount']\n",
    "dfTrump['Adverbs'] = dfTrump['RB'] + dfTrump['RBR'] + dfTrump['RBS']\n",
    "dfTrump['Adverbs/100'] = dfTrump['Adverbs'] / 100\n",
    "dfTrump['AdverbsNormalised'] = dfTrump['Adverbs'] / dfTrump['WordCount']\n",
    "dfTrump['Verbs'] = dfTrump['VB'] + dfTrump['VBD'] + dfTrump['VBG'] + dfTrump['VBN'] + dfTrump['VBP'] + dfTrump['VBZ']\n",
    "dfTrump['Verbs/100'] = dfTrump['Verbs'] / 100\n",
    "dfTrump['VerbsNormalised'] = dfTrump['Verbs'] / dfTrump['WordCount']\n",
    "dfTrump['Pronouns'] = dfTrump['PRP'] + dfTrump['PRP$']\n",
    "dfTrump['PronounsNormalised'] = dfTrump['Pronouns'] / dfTrump['WordCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to split data here\n",
    "Logic\n",
    "For dfRR where Date is between 01/01/1981 and 31/01/1985 copy data into dfRRTerm1\n",
    "For dfRR where Date is not between 01/01/1981 and 31/01/1985 copy data into dfRRTerm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReaganTerm1 = dfReagan[dfReagan.JDate < pd.Timestamp(1985, 1, 31)]\n",
    "dfReaganTerm2 = dfReagan[dfReagan.JDate > pd.Timestamp(1985, 1, 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReaganTerm1 = dfReaganTerm1.reset_index()\n",
    "dfReaganTerm1['index'] = dfReaganTerm1.index\n",
    "dfReaganTerm2 = dfReaganTerm2.reset_index()\n",
    "dfReaganTerm2['index'] = dfReaganTerm2.index\n",
    "\n",
    "dfReaganTerm1 = dfReaganTerm1.drop(['level_0'], axis=1)\n",
    "dfReaganTerm2 = dfReaganTerm2.drop(['level_0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Target Variable\n",
    "yReagan = dfReagan['index']\n",
    "yReaganTerm1 = dfReaganTerm1['index']\n",
    "yReaganTerm2 = dfReaganTerm2['index']\n",
    "yBush = dfBush['index']\n",
    "yTrump = dfTrump['index']\n",
    "\n",
    "#yRR = dfRR['Julian']\n",
    "#yGWHB = dfGWHB['Julian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset to csv files\n",
    "dfReagan.to_csv('Reagan.csv')\n",
    "dfBush.to_csv('Bush.csv')\n",
    "dfTrump.to_csv('Trump.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsReagan = pd.DataFrame()\n",
    "columnsReagan = list(dfReagan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsReagan.remove('index')\n",
    "columnsReagan.remove('Filename')\n",
    "columnsReagan.remove('Date')\n",
    "columnsReagan.remove('JDate')\n",
    "columnsReagan.remove('Julian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joe/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n"
     ]
    }
   ],
   "source": [
    "for i in columnsReagan:\n",
    "    r, p = pearsonr(dfReagan[i], dfReagan['index'])\n",
    "    pearsonResults = {'Index': i, 'RSquared':r, 'P-Value': p}\n",
    "    resultsReagan = resultsReagan.append(pearsonResults, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsBush = pd.DataFrame()\n",
    "columnsBush = list(dfBush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsBush.remove('index')\n",
    "columnsBush.remove('Filename')\n",
    "columnsBush.remove('Date')\n",
    "columnsBush.remove('JDate')\n",
    "columnsBush.remove('Julian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joe/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n"
     ]
    }
   ],
   "source": [
    "for i in columnsBush:\n",
    "    r, p = pearsonr(dfBush[i], dfBush['index'])\n",
    "    pearsonResults = {'Index': i, 'RSquared':r, 'P-Value': p}\n",
    "    resultsBush = resultsBush.append(pearsonResults, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsTrump = pd.DataFrame()\n",
    "columnsTrump = list(dfTrump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsTrump.remove('index')\n",
    "columnsTrump.remove('Filename')\n",
    "columnsTrump.remove('Date')\n",
    "columnsTrump.remove('JDate')\n",
    "columnsTrump.remove('Julian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joe/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n"
     ]
    }
   ],
   "source": [
    "for i in columnsTrump:\n",
    "    r, p = pearsonr(dfTrump[i], dfTrump['index'])\n",
    "    pearsonResults = {'Index': i, 'RSquared':r, 'P-Value': p}\n",
    "    resultsTrump = resultsTrump.append(pearsonResults, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>RSquared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>NounsNormalised</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.708304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Analytic</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.658395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NN</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.586757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UniqueWords</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.564363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Nouns/100</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.561273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Nouns</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.561273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>WDT</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>-0.520333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JJ</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>-0.518294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>article</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>-0.513836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>VBZ</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>-0.497892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Adjectives/100</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>-0.495030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Adjectives</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>-0.495030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Sixltr</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>-0.465428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>work</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>-0.454517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>AdjectivesNormalised</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>-0.406848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>-0.399314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IN</td>\n",
       "      <td>0.007386</td>\n",
       "      <td>-0.389945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NNS</td>\n",
       "      <td>0.008948</td>\n",
       "      <td>-0.381235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NNP</td>\n",
       "      <td>0.015529</td>\n",
       "      <td>-0.354851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>number</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>-0.354281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>focusfuture</td>\n",
       "      <td>0.016474</td>\n",
       "      <td>-0.351894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>VBN</td>\n",
       "      <td>0.024042</td>\n",
       "      <td>-0.332324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>WC</td>\n",
       "      <td>0.025867</td>\n",
       "      <td>-0.328397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>home</td>\n",
       "      <td>0.027763</td>\n",
       "      <td>-0.324557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CD</td>\n",
       "      <td>0.029814</td>\n",
       "      <td>-0.320638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>prep</td>\n",
       "      <td>0.030070</td>\n",
       "      <td>-0.320166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WordCount</td>\n",
       "      <td>0.030356</td>\n",
       "      <td>-0.319641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>WP$</td>\n",
       "      <td>0.030364</td>\n",
       "      <td>-0.319627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>VBP</td>\n",
       "      <td>0.031336</td>\n",
       "      <td>-0.317875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>relativ</td>\n",
       "      <td>0.032575</td>\n",
       "      <td>-0.315705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>,</td>\n",
       "      <td>0.033192</td>\n",
       "      <td>-0.314649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>risk</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>-0.308392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>WPS</td>\n",
       "      <td>0.046013</td>\n",
       "      <td>-0.295710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>anger</td>\n",
       "      <td>0.048001</td>\n",
       "      <td>0.293173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>VBD</td>\n",
       "      <td>0.045228</td>\n",
       "      <td>0.296736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>we</td>\n",
       "      <td>0.043901</td>\n",
       "      <td>0.298503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NSNouns</td>\n",
       "      <td>0.033274</td>\n",
       "      <td>0.314511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>body</td>\n",
       "      <td>0.023568</td>\n",
       "      <td>0.333385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>negate</td>\n",
       "      <td>0.022443</td>\n",
       "      <td>0.335974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>nonflu</td>\n",
       "      <td>0.019188</td>\n",
       "      <td>0.344140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>cogproc</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>0.351955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>adverb</td>\n",
       "      <td>0.015518</td>\n",
       "      <td>0.354884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>informal</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>0.357307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>FW</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>0.359800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>VerbsNormalised</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.366555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>verb</td>\n",
       "      <td>0.012164</td>\n",
       "      <td>0.366797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>hear</td>\n",
       "      <td>0.005270</td>\n",
       "      <td>0.404747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Clout</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.411670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>they</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>0.412537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>AdverbsNormalised</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.415097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>differ</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>0.418351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>focuspast</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.422839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>certain</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.457776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Dic</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.494909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>shehe</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.505852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>male</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.554273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>pronoun</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.641975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>PronounsNormalised</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.652309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>conj</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.658409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>function</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ppron</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>social</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.737041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Index  P-Value  RSquared\n",
       "143       NounsNormalised 0.000000 -0.708304\n",
       "49               Analytic 0.000001 -0.658395\n",
       "20                     NN 0.000018 -0.586757\n",
       "2             UniqueWords 0.000044 -0.564363\n",
       "142             Nouns/100 0.000050 -0.561273\n",
       "141                 Nouns 0.000050 -0.561273\n",
       "40                    WDT 0.000210 -0.520333\n",
       "16                     JJ 0.000225 -0.518294\n",
       "65                article 0.000260 -0.513836\n",
       "39                    VBZ 0.000430 -0.497892\n",
       "145        Adjectives/100 0.000470 -0.495030\n",
       "144            Adjectives 0.000470 -0.495030\n",
       "54                 Sixltr 0.001117 -0.465428\n",
       "117                  work 0.001508 -0.454517\n",
       "146  AdjectivesNormalised 0.005017 -0.406848\n",
       "13                     DT 0.005975 -0.399314\n",
       "15                     IN 0.007386 -0.389945\n",
       "23                    NNS 0.008948 -0.381235\n",
       "21                    NNP 0.015529 -0.354851\n",
       "75                 number 0.015707 -0.354281\n",
       "112           focusfuture 0.016474 -0.351894\n",
       "37                    VBN 0.024042 -0.332324\n",
       "48                     WC 0.025867 -0.328397\n",
       "119                  home 0.027763 -0.324557\n",
       "12                     CD 0.029814 -0.320638\n",
       "66                   prep 0.030070 -0.320166\n",
       "1               WordCount 0.030356 -0.319641\n",
       "46                    WP$ 0.030364 -0.319627\n",
       "38                    VBP 0.031336 -0.317875\n",
       "113               relativ 0.032575 -0.315705\n",
       "8                       , 0.033192 -0.314649\n",
       "109                  risk 0.037053 -0.308392\n",
       "53                    WPS 0.046013 -0.295710\n",
       "81                  anger 0.048001  0.293173\n",
       "35                    VBD 0.045228  0.296736\n",
       "60                     we 0.043901  0.298503\n",
       "5                 NSNouns 0.033274  0.314511\n",
       "100                  body 0.023568  0.333385\n",
       "70                 negate 0.022443  0.335974\n",
       "127                nonflu 0.019188  0.344140\n",
       "88                cogproc 0.016454  0.351955\n",
       "68                 adverb 0.015518  0.354884\n",
       "123              informal 0.014779  0.357307\n",
       "45                     FW 0.014049  0.359800\n",
       "152       VerbsNormalised 0.012226  0.366555\n",
       "71                   verb 0.012164  0.366797\n",
       "97                   hear 0.005270  0.404747\n",
       "50                  Clout 0.004477  0.411670\n",
       "63                   they 0.004385  0.412537\n",
       "149     AdverbsNormalised 0.004124  0.415097\n",
       "94                 differ 0.003812  0.418351\n",
       "110             focuspast 0.003416  0.422839\n",
       "93                certain 0.001380  0.457776\n",
       "55                    Dic 0.000471  0.494909\n",
       "62                  shehe 0.000335  0.505852\n",
       "87                   male 0.000064  0.554273\n",
       "57                pronoun 0.000002  0.641975\n",
       "154    PronounsNormalised 0.000001  0.652309\n",
       "69                   conj 0.000001  0.658409\n",
       "56               function 0.000000  0.694679\n",
       "58                  ppron 0.000000  0.716623\n",
       "83                 social 0.000000  0.737041"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetReagan = pd.DataFrame()\n",
    "subsetReagan = resultsReagan.loc[resultsReagan['P-Value'] < 0.05]\n",
    "subsetReagan1 = resultsReagan.loc[resultsReagan['RSquared'] < -0.40]\n",
    "subsetReagan2 = resultsReagan.loc[resultsReagan['RSquared'] > 0.40]\n",
    "subsetReagan1 = subsetReagan.sort_values(by=['RSquared'])\n",
    "subsetReagan2 = subsetReagan.sort_values(by=['RSquared'])\n",
    "subsetReagan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>RSquared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Dash</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.606136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>:</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.582767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Colon</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.459076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>conj</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.447959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fillers</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.357945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EX</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>-0.338771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CC</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.302863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLU</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>-0.280432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NNP</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>-0.272823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RP</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>-0.262942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>WPS</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>-0.257825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UniqueWords</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>-0.253113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>FW</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>-0.238857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PRP$</td>\n",
       "      <td>0.006046</td>\n",
       "      <td>-0.237769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>discrep</td>\n",
       "      <td>0.007429</td>\n",
       "      <td>-0.232020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IN</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>-0.226965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>SemiC</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>-0.222630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>VBD</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>-0.218828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Nouns</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>-0.214048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Nouns/100</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>-0.214048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>cogproc</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>-0.212856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>differ</td>\n",
       "      <td>0.014402</td>\n",
       "      <td>-0.212572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WordCount</td>\n",
       "      <td>0.015583</td>\n",
       "      <td>-0.210150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>WC</td>\n",
       "      <td>0.016441</td>\n",
       "      <td>-0.208487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>0.016638</td>\n",
       "      <td>-0.208115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Exclam</td>\n",
       "      <td>0.017016</td>\n",
       "      <td>-0.207415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LIVerbs</td>\n",
       "      <td>0.018469</td>\n",
       "      <td>-0.204838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JJ</td>\n",
       "      <td>0.020296</td>\n",
       "      <td>-0.201838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RB</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>-0.201548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Adverbs</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>-0.200957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Adverbs/100</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>-0.200957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MD</td>\n",
       "      <td>0.021612</td>\n",
       "      <td>-0.199818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.023429</td>\n",
       "      <td>-0.197196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NN</td>\n",
       "      <td>0.026607</td>\n",
       "      <td>-0.193002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Adjectives/100</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>-0.192878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Adjectives</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>-0.192878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>negate</td>\n",
       "      <td>0.031457</td>\n",
       "      <td>-0.187362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Pronouns</td>\n",
       "      <td>0.033147</td>\n",
       "      <td>-0.185570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>VBN</td>\n",
       "      <td>0.037478</td>\n",
       "      <td>-0.181307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>cause</td>\n",
       "      <td>0.037957</td>\n",
       "      <td>-0.180861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>VB</td>\n",
       "      <td>0.038228</td>\n",
       "      <td>-0.180611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TO</td>\n",
       "      <td>0.040403</td>\n",
       "      <td>-0.178657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PRP</td>\n",
       "      <td>0.041892</td>\n",
       "      <td>-0.177368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Verbs</td>\n",
       "      <td>0.043043</td>\n",
       "      <td>-0.176399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Verbs/100</td>\n",
       "      <td>0.043043</td>\n",
       "      <td>-0.176399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>feel</td>\n",
       "      <td>0.043785</td>\n",
       "      <td>-0.175786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>i</td>\n",
       "      <td>0.044491</td>\n",
       "      <td>-0.175210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NNS</td>\n",
       "      <td>0.047241</td>\n",
       "      <td>-0.173037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>number</td>\n",
       "      <td>0.047306</td>\n",
       "      <td>0.172987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Apostro</td>\n",
       "      <td>0.044091</td>\n",
       "      <td>0.175535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>time</td>\n",
       "      <td>0.035619</td>\n",
       "      <td>0.183083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>posemo</td>\n",
       "      <td>0.035194</td>\n",
       "      <td>0.183499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>ipron</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.190978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>``</td>\n",
       "      <td>0.022785</td>\n",
       "      <td>0.198106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTR</td>\n",
       "      <td>0.021992</td>\n",
       "      <td>0.199254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>''</td>\n",
       "      <td>0.021444</td>\n",
       "      <td>0.200070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>social</td>\n",
       "      <td>0.020914</td>\n",
       "      <td>0.200876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>affiliation</td>\n",
       "      <td>0.018641</td>\n",
       "      <td>0.204546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>verb</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>0.204996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Dic</td>\n",
       "      <td>0.013163</td>\n",
       "      <td>0.215308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>focusfuture</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.220430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>adj</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>0.223920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>VerbsNormalised</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>0.242506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>sad</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.249213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>money</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.251188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>reward</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.288671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>we</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.289680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>work</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.290711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Clout</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.294202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>home</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.295501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>drives</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.305307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Quote</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.329435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Period</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.330214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>achieve</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.356666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Index  P-Value  RSquared\n",
       "137             Dash 0.000000 -0.606136\n",
       "9                  : 0.000000 -0.582767\n",
       "133            Colon 0.000000 -0.459076\n",
       "70              conj 0.000000 -0.447959\n",
       "4            Fillers 0.000025 -0.357945\n",
       "13                EX 0.000071 -0.338771\n",
       "10                CC 0.000416 -0.302863\n",
       "3                MLU 0.001126 -0.280432\n",
       "20               NNP 0.001551 -0.272823\n",
       "28                RP 0.002319 -0.262942\n",
       "54               WPS 0.002839 -0.257825\n",
       "2        UniqueWords 0.003409 -0.253113\n",
       "46                FW 0.005812 -0.238857\n",
       "24              PRP$ 0.006046 -0.237769\n",
       "92           discrep 0.007429 -0.232020\n",
       "14                IN 0.008869 -0.226965\n",
       "134            SemiC 0.010294 -0.222630\n",
       "32               VBD 0.011707 -0.218828\n",
       "142            Nouns 0.013722 -0.214048\n",
       "143        Nouns/100 0.013722 -0.214048\n",
       "89           cogproc 0.014269 -0.212856\n",
       "95            differ 0.014402 -0.212572\n",
       "1          WordCount 0.015583 -0.210150\n",
       "49                WC 0.016441 -0.208487\n",
       "7                  , 0.016638 -0.208115\n",
       "136           Exclam 0.017016 -0.207415\n",
       "6            LIVerbs 0.018469 -0.204838\n",
       "15                JJ 0.020296 -0.201838\n",
       "25                RB 0.020481 -0.201548\n",
       "148          Adverbs 0.020862 -0.200957\n",
       "149      Adverbs/100 0.020862 -0.200957\n",
       "18                MD 0.021612 -0.199818\n",
       "12                DT 0.023429 -0.197196\n",
       "19                NN 0.026607 -0.193002\n",
       "146   Adjectives/100 0.026708 -0.192878\n",
       "145       Adjectives 0.026708 -0.192878\n",
       "71            negate 0.031457 -0.187362\n",
       "154         Pronouns 0.033147 -0.185570\n",
       "34               VBN 0.037478 -0.181307\n",
       "91             cause 0.037957 -0.180861\n",
       "31                VB 0.038228 -0.180611\n",
       "29                TO 0.040403 -0.178657\n",
       "23               PRP 0.041892 -0.177368\n",
       "151            Verbs 0.043043 -0.176399\n",
       "152        Verbs/100 0.043043 -0.176399\n",
       "99              feel 0.043785 -0.175786\n",
       "60                 i 0.044491 -0.175210\n",
       "22               NNS 0.047241 -0.173037\n",
       "76            number 0.047306  0.172987\n",
       "139          Apostro 0.044091  0.175535\n",
       "117             time 0.035619  0.183083\n",
       "79            posemo 0.035194  0.183499\n",
       "65             ipron 0.028269  0.190978\n",
       "42                `` 0.022785  0.198106\n",
       "0                TTR 0.021992  0.199254\n",
       "40                '' 0.021444  0.200070\n",
       "84            social 0.020914  0.200876\n",
       "106      affiliation 0.018641  0.204546\n",
       "72              verb 0.018378  0.204996\n",
       "56               Dic 0.013163  0.215308\n",
       "113      focusfuture 0.011092  0.220430\n",
       "73               adj 0.009850  0.223920\n",
       "153  VerbsNormalised 0.005085  0.242506\n",
       "83               sad 0.003956  0.249213\n",
       "121            money 0.003670  0.251188\n",
       "109           reward 0.000789  0.288671\n",
       "61                we 0.000754  0.289680\n",
       "118             work 0.000721  0.290711\n",
       "51             Clout 0.000617  0.294202\n",
       "120             home 0.000582  0.295501\n",
       "105           drives 0.000371  0.305307\n",
       "138            Quote 0.000115  0.329435\n",
       "131           Period 0.000110  0.330214\n",
       "107          achieve 0.000027  0.356666"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetBush = resultsBush.loc[resultsBush['P-Value'] < 0.05]\n",
    "subsetBush1 = resultsBush.loc[resultsBush['RSquared'] < -0.40]\n",
    "subsetBush2 = resultsBush.loc[resultsBush['RSquared'] > 0.40]\n",
    "subsetBush1 = subsetBush.sort_values(by=['RSquared'])\n",
    "subsetBush2 = subsetBush.sort_values(by=['RSquared'])\n",
    "subsetBush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>RSquared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Dash</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.606136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>:</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.582767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Colon</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.459076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>conj</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.447959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fillers</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.357945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EX</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>-0.338771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CC</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.302863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLU</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>-0.280432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NNP</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>-0.272823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RP</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>-0.262942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>WPS</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>-0.257825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UniqueWords</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>-0.253113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>FW</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>-0.238857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PRP$</td>\n",
       "      <td>0.006046</td>\n",
       "      <td>-0.237769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>discrep</td>\n",
       "      <td>0.007429</td>\n",
       "      <td>-0.232020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IN</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>-0.226965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>SemiC</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>-0.222630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>VBD</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>-0.218828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Nouns</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>-0.214048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Nouns/100</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>-0.214048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>cogproc</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>-0.212856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>differ</td>\n",
       "      <td>0.014402</td>\n",
       "      <td>-0.212572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WordCount</td>\n",
       "      <td>0.015583</td>\n",
       "      <td>-0.210150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>WC</td>\n",
       "      <td>0.016441</td>\n",
       "      <td>-0.208487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>0.016638</td>\n",
       "      <td>-0.208115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Exclam</td>\n",
       "      <td>0.017016</td>\n",
       "      <td>-0.207415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LIVerbs</td>\n",
       "      <td>0.018469</td>\n",
       "      <td>-0.204838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JJ</td>\n",
       "      <td>0.020296</td>\n",
       "      <td>-0.201838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RB</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>-0.201548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Adverbs</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>-0.200957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Adverbs/100</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>-0.200957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MD</td>\n",
       "      <td>0.021612</td>\n",
       "      <td>-0.199818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.023429</td>\n",
       "      <td>-0.197196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NN</td>\n",
       "      <td>0.026607</td>\n",
       "      <td>-0.193002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Adjectives/100</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>-0.192878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Adjectives</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>-0.192878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>negate</td>\n",
       "      <td>0.031457</td>\n",
       "      <td>-0.187362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Pronouns</td>\n",
       "      <td>0.033147</td>\n",
       "      <td>-0.185570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>VBN</td>\n",
       "      <td>0.037478</td>\n",
       "      <td>-0.181307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>cause</td>\n",
       "      <td>0.037957</td>\n",
       "      <td>-0.180861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>VB</td>\n",
       "      <td>0.038228</td>\n",
       "      <td>-0.180611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TO</td>\n",
       "      <td>0.040403</td>\n",
       "      <td>-0.178657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PRP</td>\n",
       "      <td>0.041892</td>\n",
       "      <td>-0.177368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Verbs</td>\n",
       "      <td>0.043043</td>\n",
       "      <td>-0.176399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Verbs/100</td>\n",
       "      <td>0.043043</td>\n",
       "      <td>-0.176399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>feel</td>\n",
       "      <td>0.043785</td>\n",
       "      <td>-0.175786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>i</td>\n",
       "      <td>0.044491</td>\n",
       "      <td>-0.175210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NNS</td>\n",
       "      <td>0.047241</td>\n",
       "      <td>-0.173037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>number</td>\n",
       "      <td>0.047306</td>\n",
       "      <td>0.172987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Apostro</td>\n",
       "      <td>0.044091</td>\n",
       "      <td>0.175535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>time</td>\n",
       "      <td>0.035619</td>\n",
       "      <td>0.183083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>posemo</td>\n",
       "      <td>0.035194</td>\n",
       "      <td>0.183499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>ipron</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.190978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>``</td>\n",
       "      <td>0.022785</td>\n",
       "      <td>0.198106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTR</td>\n",
       "      <td>0.021992</td>\n",
       "      <td>0.199254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>''</td>\n",
       "      <td>0.021444</td>\n",
       "      <td>0.200070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>social</td>\n",
       "      <td>0.020914</td>\n",
       "      <td>0.200876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>affiliation</td>\n",
       "      <td>0.018641</td>\n",
       "      <td>0.204546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>verb</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>0.204996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Dic</td>\n",
       "      <td>0.013163</td>\n",
       "      <td>0.215308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>focusfuture</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>0.220430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>adj</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>0.223920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>VerbsNormalised</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>0.242506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>sad</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.249213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>money</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.251188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>reward</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.288671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>we</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.289680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>work</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.290711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Clout</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.294202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>home</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.295501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>drives</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.305307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Quote</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.329435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Period</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.330214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>achieve</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.356666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Index  P-Value  RSquared\n",
       "137             Dash 0.000000 -0.606136\n",
       "9                  : 0.000000 -0.582767\n",
       "133            Colon 0.000000 -0.459076\n",
       "70              conj 0.000000 -0.447959\n",
       "4            Fillers 0.000025 -0.357945\n",
       "13                EX 0.000071 -0.338771\n",
       "10                CC 0.000416 -0.302863\n",
       "3                MLU 0.001126 -0.280432\n",
       "20               NNP 0.001551 -0.272823\n",
       "28                RP 0.002319 -0.262942\n",
       "54               WPS 0.002839 -0.257825\n",
       "2        UniqueWords 0.003409 -0.253113\n",
       "46                FW 0.005812 -0.238857\n",
       "24              PRP$ 0.006046 -0.237769\n",
       "92           discrep 0.007429 -0.232020\n",
       "14                IN 0.008869 -0.226965\n",
       "134            SemiC 0.010294 -0.222630\n",
       "32               VBD 0.011707 -0.218828\n",
       "142            Nouns 0.013722 -0.214048\n",
       "143        Nouns/100 0.013722 -0.214048\n",
       "89           cogproc 0.014269 -0.212856\n",
       "95            differ 0.014402 -0.212572\n",
       "1          WordCount 0.015583 -0.210150\n",
       "49                WC 0.016441 -0.208487\n",
       "7                  , 0.016638 -0.208115\n",
       "136           Exclam 0.017016 -0.207415\n",
       "6            LIVerbs 0.018469 -0.204838\n",
       "15                JJ 0.020296 -0.201838\n",
       "25                RB 0.020481 -0.201548\n",
       "148          Adverbs 0.020862 -0.200957\n",
       "149      Adverbs/100 0.020862 -0.200957\n",
       "18                MD 0.021612 -0.199818\n",
       "12                DT 0.023429 -0.197196\n",
       "19                NN 0.026607 -0.193002\n",
       "146   Adjectives/100 0.026708 -0.192878\n",
       "145       Adjectives 0.026708 -0.192878\n",
       "71            negate 0.031457 -0.187362\n",
       "154         Pronouns 0.033147 -0.185570\n",
       "34               VBN 0.037478 -0.181307\n",
       "91             cause 0.037957 -0.180861\n",
       "31                VB 0.038228 -0.180611\n",
       "29                TO 0.040403 -0.178657\n",
       "23               PRP 0.041892 -0.177368\n",
       "151            Verbs 0.043043 -0.176399\n",
       "152        Verbs/100 0.043043 -0.176399\n",
       "99              feel 0.043785 -0.175786\n",
       "60                 i 0.044491 -0.175210\n",
       "22               NNS 0.047241 -0.173037\n",
       "76            number 0.047306  0.172987\n",
       "139          Apostro 0.044091  0.175535\n",
       "117             time 0.035619  0.183083\n",
       "79            posemo 0.035194  0.183499\n",
       "65             ipron 0.028269  0.190978\n",
       "42                `` 0.022785  0.198106\n",
       "0                TTR 0.021992  0.199254\n",
       "40                '' 0.021444  0.200070\n",
       "84            social 0.020914  0.200876\n",
       "106      affiliation 0.018641  0.204546\n",
       "72              verb 0.018378  0.204996\n",
       "56               Dic 0.013163  0.215308\n",
       "113      focusfuture 0.011092  0.220430\n",
       "73               adj 0.009850  0.223920\n",
       "153  VerbsNormalised 0.005085  0.242506\n",
       "83               sad 0.003956  0.249213\n",
       "121            money 0.003670  0.251188\n",
       "109           reward 0.000789  0.288671\n",
       "61                we 0.000754  0.289680\n",
       "118             work 0.000721  0.290711\n",
       "51             Clout 0.000617  0.294202\n",
       "120             home 0.000582  0.295501\n",
       "105           drives 0.000371  0.305307\n",
       "138            Quote 0.000115  0.329435\n",
       "131           Period 0.000110  0.330214\n",
       "107          achieve 0.000027  0.356666"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetBush1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>RSquared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>focusfuture</td>\n",
       "      <td>0.008307</td>\n",
       "      <td>-0.480664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>adj</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>-0.463114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>adverb</td>\n",
       "      <td>0.019732</td>\n",
       "      <td>-0.430535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>AdverbsNormalised</td>\n",
       "      <td>0.049187</td>\n",
       "      <td>-0.368497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>motion</td>\n",
       "      <td>0.049267</td>\n",
       "      <td>-0.368377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NNPS</td>\n",
       "      <td>0.034769</td>\n",
       "      <td>0.393352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>power</td>\n",
       "      <td>0.034658</td>\n",
       "      <td>0.393574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>OtherP</td>\n",
       "      <td>0.033605</td>\n",
       "      <td>0.395701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CD</td>\n",
       "      <td>0.022845</td>\n",
       "      <td>0.421288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>number</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>0.435983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>:</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>0.446927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>$</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.496601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Dash</td>\n",
       "      <td>0.006094</td>\n",
       "      <td>0.496990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>they</td>\n",
       "      <td>0.005735</td>\n",
       "      <td>0.500095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Index  P-Value  RSquared\n",
       "112        focusfuture 0.008307 -0.480664\n",
       "72                 adj 0.011407 -0.463114\n",
       "68              adverb 0.019732 -0.430535\n",
       "149  AdverbsNormalised 0.049187 -0.368497\n",
       "114             motion 0.049267 -0.368377\n",
       "21                NNPS 0.034769  0.393352\n",
       "107              power 0.034658  0.393574\n",
       "140             OtherP 0.033605  0.395701\n",
       "11                  CD 0.022845  0.421288\n",
       "75              number 0.018067  0.435983\n",
       "9                    : 0.015073  0.446927\n",
       "43                   $ 0.006140  0.496601\n",
       "136               Dash 0.006094  0.496990\n",
       "63                they 0.005735  0.500095"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetTrump = pd.DataFrame()\n",
    "subsetTrump = resultsTrump.loc[resultsTrump['P-Value'] < 0.05]\n",
    "subsetTrump = subsetTrump.sort_values(by=['RSquared'])\n",
    "subsetTrump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Length of Utterance over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['MLU']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.ylim((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Length of Utterance over time for GWHB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['MLU']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.ylim((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Length of Utterance over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['MLU']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.ylim((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Unique Words over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['UniqueWords']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.ylim((0, 1200))\n",
    "plt.suptitle('Ronald Reagan - Unique Words')\n",
    "plt.savefig('RRUniqueWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Unique Words over time for GWHB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['UniqueWords']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)\n",
    "plt.suptitle('George Bush Snr - Unique Words')\n",
    "plt.savefig('GWHBUniqueWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Unique Words over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['UniqueWords']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Token Type Ratio over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['TTR']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Token Type Ratio over time for GWHB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['TTR']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Token Type Ratio over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['TTR']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['WordCount']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['WordCount']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['WordCount']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1)\n",
    "plt.plot(xs, ys, 'o')\n",
    "plt.plot(xs, trend[1] + trend[0] * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Nouns per 100 over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['Nouns/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Nouns per 100 over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Nouns/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Nouns per 100 over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Nouns/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Verbs per 100 over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['Verbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Verbs per 100 over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Verbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Verbs per 100 over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Verbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adjectives per 100 over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['Adjectives/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adjectives per 100 over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Adjectives/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adjectives per 100 over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Adjectives/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adverbs per 100 over time for RR\n",
    "xs = dfRR['index']\n",
    "ys = dfRR['Adverbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adverbs per 100 over time for GHWB\n",
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Adverbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adverbs per 100 over time for DJT\n",
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Adverbs/100']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['NounsNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('RR - Nouns Normalised')\n",
    "plt.savefig('RRNounsNorm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['NounsNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('GWHB - Nouns Normalised')\n",
    "plt.savefig('GWHBNounsNorm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['NounsNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('DJT - Nouns Normalised')\n",
    "plt.savefig('DJTNounsNorm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['AdjectivesNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['AdjectivesNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['AdjectivesNormalised']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['Fillers']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['Fillers']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['Fillers']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['NSNouns']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('Ronald Reagan - Non Specific Nouns')\n",
    "plt.savefig('RRNSNouns.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['NSNouns']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['NSNouns']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)\n",
    "plt.suptitle('Donald Trump - Non Specific Nouns')\n",
    "plt.savefig('DJTNSNouns.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfRR['index']\n",
    "ys = dfRR['LIVerbs']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfGWHB['index']\n",
    "ys = dfGWHB['LIVerbs']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dfDJT['index']\n",
    "ys = dfDJT['LIVerbs']\n",
    "xs = np.asarray(xs)\n",
    "trend = np.polyfit(xs, ys, 1) # fit a straight line\n",
    "plt.plot(xs, ys,'o')\n",
    "plt.plot(xs,trend[1]+trend[0]*xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsRR = dfRR['Filename']\n",
    "dfRR = dfRR.drop(['Filename', 'JDate', 'Julian', 'Date', 'index'], axis=1)\n",
    "labelsGHWB = dfGWHB['Filename']\n",
    "dfGWHB = dfGWHB.drop(['Filename', 'JDate', 'Julian', 'Date', 'index'], axis=1)\n",
    "labelsDJT = dfDJT['Filename']\n",
    "dfDJT = dfDJT.drop(['Filename', 'JDate', 'Julian', 'Date', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "yRRscaled = preprocessing.scale(yRR)\n",
    "dfRRscaled = preprocessing.scale(dfRR)\n",
    "\n",
    "yGWHBscaled = preprocessing.scale(yGWHB)\n",
    "dfGWHBscaled = preprocessing.scale(dfGWHB)\n",
    "\n",
    "yDJTscaled = preprocessing.scale(yDJT)\n",
    "dfDJTscaled = preprocessing.scale(dfDJT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- LINEAR REGRESSION --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridRR = pd.DataFrame()\n",
    "for i in range (1,1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfRRscaled, yRR, test_size=0.20)\n",
    "    # fit a model\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(X_train, y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(predictions, y_test)\n",
    "    resultsTuple = {'Slope': slope, 'intercept': intercept, 'r_value': r_value,\n",
    "                    'p_value':p_value, \n",
    "                    'std_err':std_err}\n",
    "    resultsGridRR = resultsGridRR.append(resultsTuple, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridGWHB = pd.DataFrame()\n",
    "for i in range (1,1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfGWHBscaled, yGWHB, test_size=0.20)\n",
    "    # fit a model\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(X_train, y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(predictions, y_test)\n",
    "    resultsTuple = {'Slope': slope, 'intercept': intercept, 'r_value': r_value,\n",
    "                    'p_value':p_value, \n",
    "                    'std_err':std_err}\n",
    "    resultsGridGWHB = resultsGridGWHB.append(resultsTuple, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridDJT = pd.DataFrame()\n",
    "for i in range (1,1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfDJTscaled, yDJT, test_size=0.20)\n",
    "    # fit a model\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(X_train, y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(predictions, y_test)\n",
    "    resultsTuple = {'Slope': slope, 'intercept': intercept, 'r_value': r_value,\n",
    "                    'p_value':p_value, \n",
    "                    'std_err':std_err}\n",
    "    resultsGridDJT = resultsGridDJT.append(resultsTuple, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridRR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridGWHB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGridDJT.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- GAUSSIAN PROCESSES --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/CAMPUS/alcantaj/Dropbox/'\n",
    "path = '/Users/Joe/Documents/Coding/'\n",
    "bush_df = pd.read_csv(path + 'Bush.csv')\n",
    "reagan_df = pd.read_csv(path + 'Reagan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = reagan_df['Julian'].min() # Smallest Julian Date\n",
    "maximum = reagan_df['Julian'].max() # Maximum Julian Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['NormalisedDate'] = (reagan_df['Julian'] - minimum) / (maximum - minimum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['CountDays'] = reagan_df['Julian'] - minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['CountDaysN'] = reagan_df['CountDays'] / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO DROP DATA COLUMNS HERE BEFORE RUN MODEL\n",
    "reagan_df = reagan_df.drop(['Unnamed: 0', 'Filename', 'index','Date', 'JDate', 'Julian',\n",
    "                           'NormalisedDate', 'CountDays'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = reagan_df['CountDaysN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target = target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Gaussian Process model\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(185, (1e-2, 1e4))\n",
    "# kernel = RBF(10, (1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel = kernel, n_restarts_optimizer=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(reagan_df, target, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.asarray(y_train)\n",
    "variables = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "model = gp.fit(variables, target) # This looks fine according to documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigma = gp.predict(np.asarray(X_test), return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot\n",
    "plt.figure()\n",
    "xs = [x for x in range(0, 9)]\n",
    "ys = [x for x in range(0, 9)]\n",
    "plt.plot(xs, ys)\n",
    "plt.plot(y_test, y_pred, 'o', linestyle='None')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis(xscale = 0, yscale = 0)\n",
    "plt.errorbar(y_test, y_pred, yerr=1.9600 * sigma, elinewidth=1, fillstyle='full', linestyle ='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqErr = (y_test - y_pred)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y_test':y_test, 'y_pred':y_pred, 'sigma': sigma, 'Squared Error': sqErr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = sum(sqErr) / len(sqErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bush_df = pd.read_csv(path + 'Bush.csv')\n",
    "reagan_df = pd.read_csv(path + 'Reagan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum = reagan_df['Julian'].min() # Smallest Julian Date\n",
    "maximum = reagan_df['Julian'].max() # Maximum Julian Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['NormalisedDate'] = (reagan_df['Julian'] - minimum) / (maximum - minimum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['CountDays'] = reagan_df['Julian'] - minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagan_df['CountDaysN'] = reagan_df['CountDays'] / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO DROP DATA COLUMNS HERE BEFORE RUN MODEL\n",
    "reagan_df = reagan_df.drop(['Unnamed: 0', 'Filename', 'index','Date', 'JDate', 'Julian',\n",
    "                           'NormalisedDate', 'CountDays'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = reagan_df['CountDaysN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target = target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Gaussian Process model\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(191, (1e-2, 1e4))\n",
    "# kernel = RBF(10, (1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel = kernel, n_restarts_optimizer=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reagan_df, target, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = pd.DataFrame()\n",
    "for i in range(0, 999):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reagan_df, target, test_size=0.20)\n",
    "    targetVar = np.asarray(y_train)\n",
    "    variables = np.asarray(X_train)\n",
    "    model = gp.fit(variables, targetVar)\n",
    "    y_pred, sigma = gp.predict(np.asarray(X_test), return_std=True)\n",
    "    sqErr = (y_test - y_pred)**2\n",
    "    MSE = sum(sqErr)/len(sqErr)\n",
    "    tuple = {'MSE': MSE, 'gp.kernel':gp.kernel, 'gp.kernel_':gp.kernel_}\n",
    "    Results = Results.append(tuple, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean = Results['MSE'].mean()\n",
    "SD = Results['MSE'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Mean is', Mean)\n",
    "print('The SD is', SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
