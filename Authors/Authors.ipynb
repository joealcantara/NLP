{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "# Import module with custom functions\n",
    "from lex_processing import * \n",
    "\n",
    "# Import Numpy and Pandas to work with dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import os for access to files\n",
    "import os\n",
    "\n",
    "# Import stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Import Counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path is where the data I want to process is.\n",
    "# For Mac\n",
    "# pathAC = '/Users/Joe/dropbox/Data/Original Data/Three Authors/Agatha Christie/'\n",
    "# pathIM = '/Users/Joe/dropbox/Data/Original Data/Three Authors/Iris Murdoch/'\n",
    "# pathPDJ = '/Users/Joe/dropbox/Data/Original Data/Three Authors/P James/'\n",
    "\n",
    "# For Linux\n",
    "pathAC = '/home/CAMPUS/alcantaj/Dropbox/Data/Original Data/Three Authors/Agatha Christie/'\n",
    "pathIM = '/home/CAMPUS/alcantaj/Dropbox/Data/Original Data/Three Authors/Iris Murdoch/'\n",
    "pathPDJ = '/home/CAMPUS/alcantaj/Dropbox/Data/Original Data/Three Authors/P James/'\n",
    "\n",
    "# For Windows\n",
    "# pathReagan = '/Users/jomar/Dropbox/Data/Edited Data/Presidents Data/ReaganSpeeches/'\n",
    "# pathBush = '/Users/jomar/Dropbox/Data/Edited Data/Presidents Data/BushSpeeches/'\n",
    "# pathTrump = '/Users/jomar/Dropbox/Data/Edited Data/Presidents Data/TrumpSpeeches/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Frames for the datasets. I am including one data frame for 2 terms of Reagan\n",
    "# and 2 separate dataframes for each term.\n",
    "dfAC = pd.DataFrame()\n",
    "dfPDJ = pd.DataFrame()\n",
    "dfIM = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathAC):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathAC + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('—')\n",
    "        wordsNoPunct.replace(\"—\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        words = word_tokenize(raw)\n",
    "        tokens_stemmed = [stemmer.stem(x) for x in words]\n",
    "    \n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing'] + c['everything']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)),\n",
    "                    'UniqueStems':len(set(tokens_stemmed)),\n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfAC = dfAC.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathIM):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathIM + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('—')\n",
    "        wordsNoPunct.replace(\"—\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        words = word_tokenize(raw)\n",
    "        tokens_stemmed = [stemmer.stem(x) for x in words]\n",
    "    \n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing'] + c['everything']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)),\n",
    "                    'UniqueStems':len(set(tokens_stemmed)),\n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfIM = dfIM.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathPDJ):\n",
    "    if filename.endswith('txt'):\n",
    "        f = open(pathPDJ + filename)\n",
    "        raw = f.read()\n",
    "        # Clear raw of punctuation and tokenize for word counts.\n",
    "        wordsNoPunct = strip_punctuation(raw)\n",
    "        #hesitations = wordsNoPunct.count('—')\n",
    "        wordsNoPunct.replace(\"—\", ' ')\n",
    "        wordsNoPunct = word_tokenize(wordsNoPunct)\n",
    "        words = word_tokenize(raw)\n",
    "        tokens_stemmed = [stemmer.stem(x) for x in words]\n",
    "    \n",
    "        # Word Counts for certain words\n",
    "        c = Counter(words)\n",
    "        Fillers = c['well'] + c['so'] + c['basically'] + c['actually'] + c['literally'] + c['um'] + c['ah']\n",
    "        NSNouns = c['something'] + c['anything'] + c['thing'] + c['everything']\n",
    "        LIVerbs = c['be'] + c['come'] + c['do'] + c['get'] + c['give'] + c['go'] + c['have'] + c['know'] + c['look']\n",
    "        + c['make'] + c['see'] + c['tell'] + c['think'] + c['want']\n",
    "        \n",
    "        sents = sent_tokenize(raw)\n",
    "        processed = preprocess(raw)\n",
    "        lex = lexical_diversity(wordsNoPunct)\n",
    "        mls = meanLengthSentence(processed)\n",
    "        wordDict = wordCount(processed)\n",
    "        thetuple = {'Filename': filename, 'TTR': lex,\n",
    "                    'WordCount':len(wordsNoPunct), \n",
    "                    'UniqueWords':len(set(wordsNoPunct)),\n",
    "                    'UniqueStems':len(set(tokens_stemmed)),\n",
    "                    'MLU': mls, 'Fillers': Fillers,\n",
    "                   'NSNouns': NSNouns, 'LIVerbs': LIVerbs}\n",
    "        finalDict = {**thetuple, **wordDict}\n",
    "        dfPDJ = dfPDJ.append(finalDict, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv('/home/CAMPUS/alcantaj/Dropbox/Data/Original Data/Three Authors/novels - author age and year published.csv')\n",
    "LIWC = pd.read_csv('/home/CAMPUS/alcantaj/Dropbox/Data/Original Data/Three Authors/LIWC2015 Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPDJ = pd.merge(dfPDJ, LIWC, on='Filename', how='inner')\n",
    "dfIM = pd.merge(dfIM, LIWC, on='Filename', how='inner')\n",
    "dfAC = pd.merge(dfAC, LIWC, on='Filename', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPDJ = pd.merge(dfPDJ, names, on='Filename', how='inner')\n",
    "dfIM = pd.merge(dfIM, names, on='Filename', how='inner')\n",
    "dfAC = pd.merge(dfAC, names, on='Filename', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA's with 0s as in this dataset, NAN represent the feature NOT occuring in a particular document.\n",
    "dfPDJ = dfPDJ.fillna(0)\n",
    "dfIM = dfIM.fillna(0)\n",
    "dfAC = dfAC.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPDJ = dfPDJ.sort_values(by=['Year of Publication'])\n",
    "dfIM = dfIM.sort_values(by=['Year of Publication'])\n",
    "dfAC = dfAC.sort_values(by=['Year of Publication'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfPDJ['Nouns'] = dfPDJ['NN'] + dfPDJ['NNS']+ dfPDJ['NNP'] + dfPDJ['NNPS']\n",
    "dfPDJ['Nouns/100'] = dfPDJ['Nouns'] / 100\n",
    "dfPDJ['NounsNormalised'] = dfPDJ['Nouns'] / dfPDJ['WordCount']\n",
    "dfPDJ['Adjectives'] = dfPDJ['JJ'] + dfPDJ['JJR'] + dfPDJ['JJS']\n",
    "dfPDJ['Adjectives/100'] = dfPDJ['Adjectives'] / 100\n",
    "dfPDJ['AdjectivesNormalised'] = dfPDJ['Adjectives'] / dfPDJ['WordCount']\n",
    "dfPDJ['Adverbs'] = dfPDJ['RB'] + dfPDJ['RBR'] + dfPDJ['RBS']\n",
    "dfPDJ['Adverbs/100'] = dfPDJ['Adverbs'] / 100\n",
    "dfPDJ['AdverbsNormalised'] = dfPDJ['Adverbs'] / dfPDJ['WordCount']\n",
    "dfPDJ['Verbs'] = dfPDJ['VB'] + dfPDJ['VBD'] + dfPDJ['VBG'] + dfPDJ['VBN'] + dfPDJ['VBP'] + dfPDJ['VBZ']\n",
    "dfPDJ['Verbs/100'] = dfPDJ['Verbs'] / 100\n",
    "dfPDJ['VerbsNormalised'] = dfPDJ['Verbs'] / dfPDJ['WordCount']\n",
    "dfPDJ['Pronouns'] = dfPDJ['PRP'] + dfPDJ['PRP$']\n",
    "dfPDJ['PronounsNormalised'] = dfPDJ['Pronouns'] / dfPDJ['WordCount']\n",
    "dfPDJ['UniqueWordsNormalised'] = dfPDJ['UniqueWords'] / dfPDJ['WordCount']\n",
    "dfPDJ['UniqueStemsNormalised'] = dfPDJ['UniqueStems'] / dfPDJ['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfIM['Nouns'] = dfIM['NN'] + dfIM['NNS']+ dfIM['NNP'] + dfIM['NNPS']\n",
    "dfIM['Nouns/100'] = dfIM['Nouns'] / 100\n",
    "dfIM['NounsNormalised'] = dfIM['Nouns'] / dfIM['WordCount']\n",
    "dfIM['Adjectives'] = dfIM['JJ'] + dfIM['JJR'] + dfIM['JJS']\n",
    "dfIM['Adjectives/100'] = dfIM['Adjectives'] / 100\n",
    "dfIM['AdjectivesNormalised'] = dfIM['Adjectives'] / dfIM['WordCount']\n",
    "dfIM['Adverbs'] = dfIM['RB'] + dfIM['RBR'] + dfIM['RBS']\n",
    "dfIM['Adverbs/100'] = dfIM['Adverbs'] / 100\n",
    "dfIM['AdverbsNormalised'] = dfIM['Adverbs'] / dfIM['WordCount']\n",
    "dfIM['Verbs'] = dfIM['VB'] + dfIM['VBD'] + dfIM['VBG'] + dfIM['VBN'] + dfIM['VBP'] + dfIM['VBZ']\n",
    "dfIM['Verbs/100'] = dfIM['Verbs'] / 100\n",
    "dfIM['VerbsNormalised'] = dfIM['Verbs'] / dfIM['WordCount']\n",
    "dfIM['Pronouns'] = dfIM['PRP'] + dfIM['PRP$']\n",
    "dfIM['PronounsNormalised'] = dfIM['Pronouns'] / dfIM['WordCount']\n",
    "dfIM['UniqueWordsNormalised'] = dfIM['UniqueWords'] / dfIM['WordCount']\n",
    "dfIM['UniqueStemsNormalised'] = dfIM['UniqueStems'] / dfIM['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some new aggregate columns\n",
    "dfAC['Nouns'] = dfAC['NN'] + dfAC['NNS']+ dfAC['NNP'] + dfAC['NNPS']\n",
    "dfAC['Nouns/100'] = dfAC['Nouns'] / 100\n",
    "dfAC['NounsNormalised'] = dfAC['Nouns'] / dfAC['WordCount']\n",
    "dfAC['Adjectives'] = dfAC['JJ'] + dfAC['JJR'] + dfAC['JJS']\n",
    "dfAC['Adjectives/100'] = dfAC['Adjectives'] / 100\n",
    "dfAC['AdjectivesNormalised'] = dfAC['Adjectives'] / dfAC['WordCount']\n",
    "dfAC['Adverbs'] = dfAC['RB'] + dfAC['RBR'] + dfIM['RBS']\n",
    "dfAC['Adverbs/100'] = dfAC['Adverbs'] / 100\n",
    "dfAC['AdverbsNormalised'] = dfAC['Adverbs'] / dfAC['WordCount']\n",
    "dfAC['Verbs'] = dfAC['VB'] + dfAC['VBD'] + dfAC['VBG'] + dfAC['VBN'] + dfAC['VBP'] + dfAC['VBZ']\n",
    "dfAC['Verbs/100'] = dfAC['Verbs'] / 100\n",
    "dfAC['VerbsNormalised'] = dfAC['Verbs'] / dfAC['WordCount']\n",
    "dfAC['Pronouns'] = dfAC['PRP'] + dfAC['PRP$']\n",
    "dfAC['PronounsNormalised'] = dfAC['Pronouns'] / dfAC['WordCount']\n",
    "dfAC['UniqueWordsNormalised'] = dfAC['UniqueWords'] / dfAC['WordCount']\n",
    "dfAC['UniqueStemsNormalised'] = dfAC['UniqueStems'] / dfAC['WordCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset to csv files\n",
    "dfPDJ.to_csv('PDJ.csv')\n",
    "dfIM.to_csv('IM.csv')\n",
    "dfAC.to_csv('AC.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
