lmPress = sum(lmresults)
vectortmp = data.frame("WDT",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# Nouns Feature
for (i in 1:(nrow(df))){
gamModel = gam(Nouns ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$Nouns[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(Nouns ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$Nouns[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("Nouns",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# Nouns.100 Feature
for (i in 1:(nrow(df))){
gamModel = gam(Nouns.100 ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$Nouns.100[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(Nouns.100 ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$Nouns.100[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("Nouns/100",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# UniqueStems Feature
for (i in 1:(nrow(df))){
gamModel = gam(UniqueStems ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$UniqueStems[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(UniqueStems ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$UniqueStems[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("UniqueStems",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# shehe Feature
for (i in 1:(nrow(df))){
gamModel = gam(shehe ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$shehe[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(shehe ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$shehe[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("shehe",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# VBZ Feature
for (i in 1:(nrow(df))){
gamModel = gam(VBZ ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$VBZ[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(VBZ ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$VBZ[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("VBZ",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# JJ Feature
for (i in 1:(nrow(df))){
gamModel = gam(JJ ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$JJ[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(JJ ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$JJ[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("JJ",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# article Feature
for (i in 1:(nrow(df))){
gamModel = gam(article ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$article[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(article ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$article[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("article",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# Adjectives Feature
for (i in 1:(nrow(df))){
gamModel = gam(Adjectives ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$Adjectives[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(Adjectives ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$Adjectives[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("Adjectives",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# Adjectives.100 Feature
for (i in 1:(nrow(df))){
gamModel = gam(Adjectives.100 ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$Adjectives.100[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(Adjectives.100 ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$Adjectives.100[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("Adjectives.100",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
# Dic Feature
for (i in 1:(nrow(df))){
gamModel = gam(Dic ~ s(Days, bs = "gp", k = 40),
data = df[-i, ],
family = Gamma(link = "log"),
method = "REML")
pred = predict.gam(gamModel, df[i, ],
type = "response")
results[i] = (as.numeric(pred) - df$Dic[i]) ^ 2
}
# PRESS
gamPress = sum(results)
lmresults = numeric(nrow(df))
for (i in 1:(nrow(df))){
linearModel = lm(Dic ~ Days, data = df[-i, ])
pred = predict.lm(linearModel, df[i, ], type = "response")
lmresults[i] = (as.numeric(pred) - df$Dic[i]) ^ 2
}
# PRESS
lmPress = sum(lmresults)
vectortmp = data.frame("Dic",gamPress, lmPress)
names(vectortmp) = c('feature','gamPRESS', 'lmPRESS')
totalResults = rbind(totalResults, vectortmp)
test = totalResults
test = test %>%
mutate_at(c(2,3), funs(c(scale(.))))
ttest = t.test(totalResults[,2], totalResults[,3], paired = TRUE)
indx <- totalResults[,2] < totalResults[,3]
binom.test(sum(indx), length(indx))
ttest
scatter_plot <- ggplot(dataReagan, aes(Days, ppron))
scatter_plot +  geom_point() + labs(x = "Days", y = "ppron") + geom_smooth(method="lm", color = "black", lty = 1) + geom_smooth(method="loess", color = "black", lty = 2) + theme_gray()
ggsave('comparison1.png')
# Linear models, comparison from first point to all points after 700 days
df = dataReagan
df2 = df
df = filter(dataReagan, Days > 700 | Days < 1)
df2 = filter(dataReagan, Days > 700 | Days < 37 & Days > 2)
df = df %>% select(Days, ppron)
df2 = df2 %>% select(Days, ppron)
df$diff = df$ppron - 8.86
df2$diff = df2$ppron - 9.44
df$status = 'stable'
df$status[df$diff > 0.10] = 'declining'
df$status[df$diff < -0.10] = 'improving'
df2$status = 'stable'
df2$status[df2$diff > 0.10] = 'declining'
df2$status[df2$diff < -0.10] = 'improving'
ggplot(df, aes(x=Days, y=ppron)) +
geom_point() +
geom_segment(aes(x = 0, y = 8.83, xend = df$Days, yend = df$ppron, linetype = status), data = df)
ggsave('comp1.png')
ggplot(df2, aes(x=Days, y=ppron)) +
geom_point() +
geom_segment(aes(x = 36, y = 9.44, xend = df2$Days, yend = df2$ppron, linetype = status), data = df2)
ggsave('comp2.png')
NBA_FULL <- read.csv("~/Documents/Personal/ARCHIVE - PHD WORK/ARCHIVE - Non PhD/ProjectSubmission/Data/NBA_FULL.csv")
View(NBA_FULL)
rm(list=ls()) # Clear Workspace
set.seed(7410) # Set Seed
# Load libraries
library(tsne)
library(ggplot2)
library(RColorBrewer)
###--- Define all functions needed ---###
# Normalize function - used to normalize values
normalize = function (x) {
return ((x - mean(x)) / sd(x))
}
# Euclidean distance function - used to calculate the euclidean distance between two vectors
euclidean = function (x, y) {
return (sum((x - y) ^ 2))
}
# Modified Pearson correlation
pearson = function(x,y){
x = as.numeric(x)
y = as.numeric(y)
r = cor(x,y)
return(1-r)
}
cosine = function(x,y) {
x = as.numeric(x)
y = as.numeric(y)
inner = sum(x*y)
res = inner/sqrt(sum(x^2))/sqrt(sum(y^2))
return(1-res)
}
# Forgy initialisation.
Forgy = function() {
r.points = sample(1:nrow(normalized.df),k,replace=FALSE)
centroids = normalized.df[r.points,]
return(centroids)
}
# Random K initialisation.
randomK = function() {
for (i in 1:n_feat){
centroids[,i] = runif(k, min = min(normalized.df[,i], max = max(normalized.df[,i])))
}
return(centroids)
}
# Random partitions initialisation.
randomPart = function() {
allocation = sample(1:k,n_obs,replace=TRUE);
for (i in 1:k) {
centroids[i,] = colMeans(normalized.df[allocation==i,])
}
return(centroids)
}
# k means++ initialization.
# Choose one center uniformly at random from among the data points.
kmeansplus = function() {
found = 0
point = sample(1:n_obs,1)[1]
while(found < k) {
found = found+1
centroids[found,] = as.matrix(normalized.df[point,],nrow=1,ncol=n_feat)
# For each data point x, compute D(x),
# the distance between x and the nearest center that has already been chosen.
init_dist = rep(0,n_obs);
for (j in 1:n_obs) {
point_curr = normalized.df[j,] # current point
bestv = Inf # best distance
bestc = 0 # closest centroid
for (i in 1:found) {
centroid_curr = centroids[i,]
diff = euclidean(point_curr,centroid_curr)
#cat(diff,'\n')
if( diff < bestv){
bestv =diff
bestc = i
}
}
init_dist[j] = bestv
}
rand_num = runif(1)
cdf = cumsum(init_dist)/sum(init_dist)
point = which(cdf >= rand_num)[1]
#cat('assigned to point',point,'\n')
}
return(centroids)
}
###--- Load Datasets ---###
original_data = read.csv("~/Documents/Personal/ARCHIVE - PHD WORK/ARCHIVE - Non PhD/ProjectSubmission/Data/NBA_FULL.csv")
###--- PreProcess ---###
# Take out any player who played less than 20 games.
processed_data = original_data[original_data$G >= 20, ]
# Separate datasets into data to be clustered and labels
# df = original_data[3:11]
processed_data = processed_data[, c("Player", "Year", "Pos", "TS.", "AST.", "FG.", "FT.", "X3P.", "X2P.",
"ORB", "DRB", "AST", "STL", "BLK", "TOV", "PF", "PTS", "Player_year")]
# Take any observations out which have incomplete values
processed_data = processed_data[complete.cases(processed_data),]
df = processed_data[, c("TS.", "AST.", "FG.", "FT.", "X3P.", "X2P.",
"ORB", "DRB", "AST", "STL", "BLK", "TOV", "PF", "PTS")]
names = processed_data[1:3]
# Create copy of the data to be normalized.
normalized.df = df
###--- Data pre-processing ---###
normalized.df = as.data.frame(apply(normalized.df,2,normalize))
###--- Data Exploration and Summary Stats ---###
# Number of positions in each position
table(original_data$Pos)
plot(original_data$Pos)
# Histogram of players and the number of games they've played
hist(original_data$GS)
# Histgram of players by year
table(original_data$Year)
plot(original_data$Year)
n_obs = nrow(df) # Count number of rows in the dataset
n_feat = ncol(df) # Count number of features in the dataset
o_feat = ncol(processed_data) # Count number of features in the dataset
n_iterations = 500 # Set max number of iterations to complete clustering
###--- Initialise centroids matrix  ---###
centroids = matrix(0, nrow=k, ncol=n_feat)
## Initialize distances between points and centroids - centroids are in columns
distances = matrix(0,nrow=nrow(normalized.df),ncol=k)
# the grid search
results = expand.grid(init = c('randomPart'),
distance= c('euclidean'),
numClust = c(12), stringsAsFactors = FALSE)
# number of rows in the grid search
n_results = nrow(results)
# assign names to each of the combinations
result_names = paste(results$init, results$distance, results$numClust, sep = "_")
results = cbind(result_names, results)
# vector to store the SSE for each combination
SSES = rep(0,n_results)
# vector to store silhouette for each combination
silh = rep(0,n_results)
## Initialize cluster membership matrix for each combination
inCluster = matrix(0,nrow=n_results,ncol=n_obs)
for (row in 1:nrow(results)) {
params = results[row,]
cat('\n\nInitialization method:',params$init,'\n')
cat('Distance used:',params$distance,'\n')
cat('k=',params$numClust,'\n')
cat('--------------------------------------------\n\n')
k = params$numClust
###--- Initialise centroids matrix  ---###
centroids = matrix(0, nrow=k, ncol=n_feat)
## Initialize distances between points and centroids - centroids are in columns
distances = matrix(0,nrow=nrow(normalized.df),ncol=k)
centroids = do.call(params$init,args=list())
## assignment to centroids
assign.old = rep(0,nrow(normalized.df))
assign = rep(1,nrow(normalized.df))
count = 0
while( sum(assign != assign.old) > 1) {
count = count+1
cat('iteration: ',count,'\n')
#cat('differences: ',sum(assign != assign.old), '\n')
for(j in 1:k)
distances[,j] = apply(normalized.df,1,function(x) return(do.call(params$distance,args=list(x,centroids[j,]))))
### assignments
assign.old = assign
assign = apply(distances,1,which.min)
### update centroids
for(i in 1:k)
centroids[i, ] = colSums(normalized.df[assign==i,])/sum(assign==i)
}
sse = 0
for( i in 1:k) {
sse = sse + sum(apply(normalized.df[which(assign==i),],1,function(x) return(euclidean(x,centroids[i,]))))
}
cat('SSE=',sse,'\n')
SSES[row] = sse
inCluster[row,] = assign
}
# add the vectors of SSEs to the results dataframe
results = cbind(results,SSE=SSES)
# Take the inverse of the clusters identified
newCluster = as.data.frame(t(inCluster))
# Rename clusters
names(newCluster) = results$result_names
# Assign original data to a new data frame for analysis
new.df = processed_data
# Add the new clusters to the new.df
new.df = cbind(new.df, newCluster)
new.df_col = as.numeric(ncol(new.df))
# write to csv files for analysis
write.csv(new.df, file = "final_clusters.csv")
###--- Output to file ---###
sink("final_cluster.txt", append=FALSE, split=TRUE)
for (j in (o_feat+1):new.df_col) {
print(table(new.df$Pos, new.df[,j]))
}
for (i in 1:k) {
print(new.df$Player_year[new.df$randomPart_euclidean_12==i])
print(apply(new.df[new.df$randomPart_euclidean_12==i,4:17],2,mean))
}
for (j in (o_feat+1):new.df_col) {
for(i in 1:k) {
cat('\nCluster ',i,'\n')
centre_of_cluster = colMeans(new.df[new.df[,j]==i,4:17])
print(centre_of_cluster)
# print(new.df$Player[new.df[,j]==i])
if(!is.nan(centre_of_cluster)) {
###--- Attempt to find closest player to the cluster centre ---###
minimum = Inf
neighbour = NA
for (i in 1:n_obs) {
dist = euclidean(centre_of_cluster, df[i,])
if(dist < minimum){
minimum = dist # Update to new minimum value
neighbour = i # Update index of new minimum value
}
}
cat('Exemplar of cluster')
print(processed_data[neighbour,])
cat('Cluster Centre')
print(centre_of_cluster)
cat('Distance from player to centre of cluster')
print(minimum)
}
}
}
sink()
# Clear Workspace
rm(list=ls())
# Clear Workspace
rm(list=ls())
# Load Libraries
library(ggplot2)
# Load Data
PDJ <- read.csv("~/Documents/NLP/Authors/PDJ.csv")
AC <- read.csv("~/Documents/NLP/Authors/AC.csv")
IM <- read.csv("~/Documents/NLP/Authors/IM.csv")
scatter_plot <- ggplot(AC, aes(Year.of.Publication , TTR))
scatter_plot +  geom_point() + labs(x = "Year", y = "ppron") + geom_smooth(method="lm", color = "black", lty = 1) + geom_smooth(method="loess", color = "black", lty = 2) + theme_gray()
scatter_plot <- ggplot(IM, aes(Year.of.Publication , TTR))
scatter_plot +  geom_point() + labs(x = "Year", y = "ppron") + geom_smooth(method="lm", color = "black", lty = 1) + geom_smooth(method="loess", color = "black", lty = 2) + theme_gray()
scatter_plot <- ggplot(PDJ, aes(Year.of.Publication , TTR))
scatter_plot +  geom_point() + labs(x = "Year", y = "ppron") + geom_smooth(method="lm", color = "black", lty = 1) + geom_smooth(method="loess", color = "black", lty = 2) + theme_gray()
scatter_plot <- ggplot(AC, aes(Year.of.Publication , UniqueStems))
scatter_plot +  geom_point() + labs(x = "Year", y = "UniqueStems") + geom_smooth(method="lm", color = "black", lty = 1) + geom_smooth(method="loess", color = "black", lty = 2) + theme_gray()
scatter_plot <- ggplot(IM, aes(Year.of.Publication , UniqueStems))
scatter_plot +  geom_point() + labs(x = "Year", y = "UniqueStems") + geom_smooth(method="lm", color = "black", lty = 1) + geom_smooth(method="loess", color = "black", lty = 2) + theme_gray()
scatter_plot <- ggplot(PDJ, aes(Year.of.Publication , UniqueStems))
scatter_plot +  geom_point() + labs(x = "Year", y = "UniqueStems") + geom_smooth(method="lm", color = "black", lty = 1) + geom_smooth(method="loess", color = "black", lty = 2) + theme_gray()
View(IM)
source('~/Documents/NLP/FullCodePresident.R')
